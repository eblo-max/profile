"""
üì¶ –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ô –ö–≠–®–ò–†–£–Æ–©–ò–ô –ú–ï–ù–ï–î–ñ–ï–†
–°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –Ω–∞ AI –∑–∞–ø—Ä–æ—Å–∞—Ö
"""
import asyncio
import structlog
import hashlib
import json
import redis.asyncio as redis
from typing import Any, Dict, List, Optional, Union, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict

from src.config.settings import settings

logger = structlog.get_logger()


@dataclass
class CacheEntry:
    """–ó–∞–ø–∏—Å—å –≤ –∫—ç—à–µ"""
    key: str
    data: Any
    timestamp: datetime
    ttl_hours: int
    access_count: int
    cost_saved_usd: float


class IntelligentCacheManager:
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Å—Ä–µ–¥—Å—Ç–≤"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à-–º–µ–Ω–µ–¥–∂–µ—Ä–∞"""
        self.redis_client: Optional[redis.Redis] = None
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "savings_usd": 0.0,
            "entries_count": 0
        }
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —Ç–∏–ø–∞–º
        self.cache_configs = {
            "analysis_basic": {
                "ttl_hours": settings.analysis_cache_ttl_hours,
                "prefix": "analysis:basic:",
                "cost_per_miss": 1.99
            },
            "analysis_advanced": {
                "ttl_hours": settings.analysis_cache_ttl_hours,
                "prefix": "analysis:advanced:",
                "cost_per_miss": 4.99
            },
            "scientific_research": {
                "ttl_hours": settings.scientific_cache_ttl_hours,
                "prefix": "research:scientific:",
                "cost_per_miss": 9.99
            },
            "ai_response": {
                "ttl_hours": 6,  # AI –æ—Ç–≤–µ—Ç—ã –∫—ç—à–∏—Ä—É–µ–º –Ω–∞ 6 —á–∞—Å–æ–≤
                "prefix": "ai:response:",
                "cost_per_miss": 0.50
            },
            "user_profile": {
                "ttl_hours": 24,
                "prefix": "user:profile:",
                "cost_per_miss": 0.0
            }
        }
        
        logger.info("üì¶ IntelligentCacheManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω",
                   cache_types=len(self.cache_configs),
                   default_ttl_hours=settings.analysis_cache_ttl_hours)
    
    async def initialize(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Redis"""
        try:
            self.redis_client = redis.from_url(
                settings.redis_url,
                decode_responses=True,
                socket_timeout=5.0,
                socket_connect_timeout=5.0
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            await self.redis_client.ping()
            logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ", 
                       redis_url=settings.redis_url[:20] + "...")
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Redis", 
                        error=str(e), 
                        exc_info=True)
            self.redis_client = None
    
    def _generate_cache_key(
        self, 
        cache_type: str, 
        identifier: str,
        extra_params: Optional[Dict[str, Any]] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–º–Ω–æ–≥–æ –∫–ª—é—á–∞ –∫—ç—à–∞"""
        config = self.cache_configs.get(cache_type, {"prefix": f"{cache_type}:"})
        
        # –ë–∞–∑–æ–≤—ã–π –∫–ª—é—á
        base_key = f"{config['prefix']}{identifier}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ —Ö—ç—à
        if extra_params:
            params_str = json.dumps(extra_params, sort_keys=True)
            params_hash = hashlib.md5(params_str.encode()).hexdigest()[:8]
            base_key += f":{params_hash}"
        
        return base_key
    
    def _generate_content_hash(self, content: Union[str, Dict[str, Any]]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ö—ç—à–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if isinstance(content, dict):
            content_str = json.dumps(content, sort_keys=True)
        else:
            content_str = str(content)
        
        return hashlib.sha256(content_str.encode()).hexdigest()[:16]
    
    async def get_cached_analysis(
        self, 
        text: str, 
        analysis_level: str,
        user_id: Optional[int] = None
    ) -> Optional[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ —Ç–µ–∫—Å—Ç—É
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analysis_level: –£—Ä–æ–≤–µ–Ω—å –∞–Ω–∞–ª–∏–∑–∞ (basic, advanced, research)
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–ª–∏ None
        """
        if not self.redis_client:
            return None
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö—ç—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_hash = self._generate_content_hash(text)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á –∫—ç—à–∞
            cache_key = self._generate_cache_key(
                f"analysis_{analysis_level}",
                content_hash,
                {"user_id": user_id} if user_id else None
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Redis
            cached_data = await self.redis_client.get(cache_key)
            
            if cached_data:
                result = json.loads(cached_data)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self.cache_stats["hits"] += 1
                cost_saved = self.cache_configs.get(
                    f"analysis_{analysis_level}", 
                    {"cost_per_miss": 1.0}
                )["cost_per_miss"]
                self.cache_stats["savings_usd"] += cost_saved
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–æ—Å—Ç—É–ø–∞
                await self._increment_access_count(cache_key)
                
                logger.info("üéØ –ö—ç—à –ø–æ–ø–∞–¥–∞–Ω–∏–µ",
                           cache_key=cache_key[:50] + "...",
                           analysis_level=analysis_level,
                           cost_saved_usd=cost_saved)
                
                return result
            else:
                self.cache_stats["misses"] += 1
                logger.debug("‚ùå –ö—ç—à –ø—Ä–æ–º–∞—Ö", 
                           cache_key=cache_key[:50] + "...",
                           analysis_level=analysis_level)
                return None
                
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞", 
                        analysis_level=analysis_level,
                        error=str(e), 
                        exc_info=True)
            return None
    
    async def cache_analysis_result(
        self, 
        text: str,
        level: str,
        result: str,
        user_id: Optional[int] = None,
        custom_ttl_hours: Optional[int] = None
    ) -> bool:
        """
        –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ (—Å—Ç—Ä–æ–∫–∞)
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            level: –£—Ä–æ–≤–µ–Ω—å –∞–Ω–∞–ª–∏–∑–∞
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è (—Å—Ç—Ä–æ–∫–∞)
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            custom_ttl_hours: –ö–∞—Å—Ç–æ–º–Ω–æ–µ –≤—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ
        """
        if not self.redis_client:
            return False
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö—ç—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_hash = self._generate_content_hash(text)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á –∫—ç—à–∞
            cache_key = self._generate_cache_key(
                f"analysis_{level}",
                content_hash,
                {"user_id": user_id} if user_id else None
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º TTL
            cache_config = self.cache_configs.get(f"analysis_{level}")
            ttl_hours = custom_ttl_hours or (cache_config["ttl_hours"] if cache_config else 24)
            ttl_seconds = ttl_hours * 3600
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
            cached_result = {
                "data": result,  # –°—Ç—Ä–æ–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
                "metadata": {
                    "cached_at": datetime.utcnow().isoformat(),
                    "analysis_level": level,
                    "content_hash": content_hash,
                    "user_id": user_id,
                    "ttl_hours": ttl_hours
                }
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Redis
            await self.redis_client.setex(
                cache_key,
                ttl_seconds,
                json.dumps(cached_result)
            )
            
            self.cache_stats["entries_count"] += 1
            
            logger.info("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω",
                       cache_key=cache_key[:50] + "...",
                       analysis_level=level,
                       ttl_hours=ttl_hours)
            
            return True
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞",
                        analysis_level=level,
                        error=str(e), 
                        exc_info=True)
            return False
    
    async def get_analysis_result(
        self, 
        text: str, 
        level: str,
        user_id: Optional[int] = None
    ) -> Optional[str]:
        """
        –ê–õ–ò–ê–° –¥–ª—è get_cached_analysis —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            level: –£—Ä–æ–≤–µ–Ω—å –∞–Ω–∞–ª–∏–∑–∞
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –°—Ç—Ä–æ–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞ –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        cached_analysis = await self.get_cached_analysis(text, level, user_id)
        
        if cached_analysis and "data" in cached_analysis:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –∏–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            analysis_data = cached_analysis["data"]
            
            # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if isinstance(analysis_data, str):
                return analysis_data
            
            # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å - –∏—â–µ–º –ø–æ–ª–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
            if isinstance(analysis_data, dict):
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–ª—é—á–∏
                for key in ["result", "analysis", "text", "content", "output"]:
                    if key in analysis_data:
                        return str(analysis_data[key])
                
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤–µ—Å—å —Å–ª–æ–≤–∞—Ä—å –≤ —Å—Ç—Ä–æ–∫—É
                return str(analysis_data)
        
        return None
    
    async def get_cached_scientific_research(
        self, 
        query_terms: List[str],
        max_sources: int = 30
    ) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        if not self.redis_client:
            return None
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Ö—ç—à –æ—Ç –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            query_hash = self._generate_content_hash({
                "terms": sorted(query_terms),
                "max_sources": max_sources
            })
            
            cache_key = self._generate_cache_key("scientific_research", query_hash)
            cached_data = await self.redis_client.get(cache_key)
            
            if cached_data:
                result = json.loads(cached_data)
                self.cache_stats["hits"] += 1
                self.cache_stats["savings_usd"] += 9.99  # –≠–∫–æ–Ω–æ–º–∏—è –Ω–∞ –Ω–∞—É—á–Ω–æ–º –ø–æ–∏—Å–∫–µ
                
                logger.info("üéØ –ù–∞—É—á–Ω—ã–π –∫—ç—à –ø–æ–ø–∞–¥–∞–Ω–∏–µ",
                           query_terms=query_terms,
                           cost_saved_usd=9.99)
                
                return result["data"]
            
            return None
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—É—á–Ω–æ–≥–æ –∫—ç—à–∞", error=str(e))
            return None
    
    async def cache_scientific_research(
        self, 
        query_terms: List[str],
        research_result: Dict[str, Any],
        max_sources: int = 30
    ) -> bool:
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–∞—É—á–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        if not self.redis_client:
            return False
        
        try:
            query_hash = self._generate_content_hash({
                "terms": sorted(query_terms),
                "max_sources": max_sources
            })
            
            cache_key = self._generate_cache_key("scientific_research", query_hash)
            ttl_seconds = settings.scientific_cache_ttl_hours * 3600
            
            cached_result = {
                "data": research_result,
                "metadata": {
                    "cached_at": datetime.utcnow().isoformat(),
                    "query_terms": query_terms,
                    "max_sources": max_sources,
                    "sources_found": len(research_result.get("sources", []))
                }
            }
            
            await self.redis_client.setex(
                cache_key,
                ttl_seconds,
                json.dumps(cached_result)
            )
            
            logger.info("üíæ –ù–∞—É—á–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ",
                       query_terms=query_terms,
                       sources_count=len(research_result.get("sources", [])),
                       ttl_hours=settings.scientific_cache_ttl_hours)
            
            return True
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞—É—á–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", error=str(e))
            return False
    
    async def _increment_access_count(self, cache_key: str) -> None:
        """–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –∫—ç—à-–∑–∞–ø–∏—Å–∏"""
        try:
            access_key = f"access_count:{cache_key}"
            await self.redis_client.incr(access_key)
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º TTL –¥–ª—è —Å—á–µ—Ç—á–∏–∫–∞ –¥–æ—Å—Ç—É–ø–∞
            await self.redis_client.expire(access_key, 7 * 24 * 3600)  # 7 –¥–Ω–µ–π
        except Exception as e:
            logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—á–µ—Ç—á–∏–∫–∞ –¥–æ—Å—Ç—É–ø–∞", error=str(e))
    
    async def get_cache_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not self.redis_client:
            return {"status": "Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"}
        
        try:
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Redis
            redis_info = await self.redis_client.info("memory")
            
            # –ü–æ–¥—Å—á–µ—Ç –∑–∞–ø–∏—Å–µ–π –ø–æ —Ç–∏–ø–∞–º
            type_stats = {}
            for cache_type, config in self.cache_configs.items():
                pattern = f"{config['prefix']}*"
                keys = await self.redis_client.keys(pattern)
                type_stats[cache_type] = len(keys)
            
            hit_rate = (
                self.cache_stats["hits"] / 
                (self.cache_stats["hits"] + self.cache_stats["misses"])
                if (self.cache_stats["hits"] + self.cache_stats["misses"]) > 0 
                else 0
            )
            
            return {
                "hit_rate": round(hit_rate, 3),
                "total_hits": self.cache_stats["hits"],
                "total_misses": self.cache_stats["misses"],
                "total_savings_usd": round(self.cache_stats["savings_usd"], 2),
                "entries_by_type": type_stats,
                "redis_memory_used": redis_info.get("used_memory_human", "N/A"),
                "cache_efficiency": "–í—ã—Å–æ–∫–∞—è" if hit_rate > 0.7 else "–°—Ä–µ–¥–Ω—è—è" if hit_rate > 0.4 else "–ù–∏–∑–∫–∞—è"
            }
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞", error=str(e))
            return {"error": str(e)}
    
    async def clear_cache_by_pattern(self, pattern: str) -> int:
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É"""
        if not self.redis_client:
            return 0
        
        try:
            keys = await self.redis_client.keys(pattern)
            if keys:
                deleted_count = await self.redis_client.delete(*keys)
                logger.info("üóëÔ∏è –ö—ç—à –æ—á–∏—â–µ–Ω", 
                           pattern=pattern, 
                           deleted_keys=deleted_count)
                return deleted_count
            return 0
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞", pattern=pattern, error=str(e))
            return 0
    
    async def warm_up_cache(self, popular_queries: List[str]) -> None:
        """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ–≤ –∫—ç—à–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
        logger.info("üî• –ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–≥—Ä–µ–≤ –∫—ç—à–∞", queries_count=len(popular_queries))
        
        for query in popular_queries:
            # –¢—É—Ç –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            # –ù–æ –ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
            logger.debug("üî• –ü—Ä–æ–≥—Ä–µ–≤ –∫—ç—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞", query=query[:50] + "...")
    
    async def close(self) -> None:
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Redis"""
        if self.redis_client:
            await self.redis_client.aclose()
            logger.info("üì¶ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis –∑–∞–∫—Ä—ã—Ç–æ")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫—ç—à-–º–µ–Ω–µ–¥–∂–µ—Ä–∞
cache_manager = IntelligentCacheManager() 