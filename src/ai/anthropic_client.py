"""
–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Anthropic Claude API
"""
import asyncio
import structlog
from typing import Dict, Any, List, Optional, AsyncGenerator
from anthropic import AsyncAnthropic
from anthropic.types import Message

from src.config.settings import settings
from src.ai.prompts.psychology_prompts import (
    PSYCHOLOGICAL_ANALYSIS_PROMPT,
    PERSONALITY_ASSESSMENT_PROMPT,
    EMOTIONAL_ANALYSIS_PROMPT,
    BEHAVIORAL_ANALYSIS_PROMPT,
    SYNTHESIS_PROMPT,
    MULTI_AI_SYNTHESIS_PROMPT
)

logger = structlog.get_logger()


class AnthropicClient:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Claude API"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞"""
        self.client = AsyncAnthropic(
            api_key=settings.anthropic_api_key,
            timeout=60.0,  # 60 —Å–µ–∫—É–Ω–¥ —Ç–∞–π–º–∞—É—Ç
            max_retries=3   # 3 –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
        )
        self.model = "claude-3-5-sonnet-latest"
        logger.info("üß† AnthropicClient –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω", model=self.model)
    
    async def analyze_text(self, 
                          text: str, 
                          analysis_type: str = "psychological", 
                          user_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Claude —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analysis_type: –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞ (psychological, personality, emotional, behavioral, synthesis)
            user_context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç Claude
        """
        try:
            logger.info("üîç –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞", 
                       text_length=len(text), 
                       analysis_type=analysis_type)
            
            # –í—ã–±–æ—Ä –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∞–Ω–∞–ª–∏–∑–∞
            prompt = self._get_prompt_for_analysis_type(text, analysis_type, user_context)
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Claude
            message = await self.client.messages.create(
                model=self.model,
                max_tokens=4000,  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                temperature=0.3,  # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞
            response_text = ""
            for content_block in message.content:
                if content_block.type == "text":
                    response_text += content_block.text
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            result = self._parse_analysis_response(response_text, analysis_type)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            result["tokens_used"] = message.usage.input_tokens + message.usage.output_tokens
            result["model_used"] = self.model
            result["analysis_timestamp"] = asyncio.get_event_loop().time()
            
            logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω", 
                       tokens_used=result["tokens_used"],
                       confidence_score=result.get('confidence_score', 0))
            
            return result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞", error=str(e), exc_info=True)
            return {
                "error": str(e),
                "analysis_type": analysis_type,
                "status": "failed"
            }
    
    def _get_prompt_for_analysis_type(self, 
                                     text: str, 
                                     analysis_type: str,
                                     user_context: Optional[Dict[str, Any]] = None) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        
        context_str = str(user_context) if user_context else "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω"
        
        # –í—ã–±–æ—Ä –ø—Ä–æ–º–ø—Ç–∞
        if analysis_type == "psychological" or analysis_type == "comprehensive_psychological":
            return PSYCHOLOGICAL_ANALYSIS_PROMPT.format(text=text, context=context_str)
        elif analysis_type == "personality":
            return PERSONALITY_ASSESSMENT_PROMPT.format(text=text, context=context_str)
        elif analysis_type == "emotional":
            return EMOTIONAL_ANALYSIS_PROMPT.format(text=text, context=context_str)
        elif analysis_type == "behavioral":
            return BEHAVIORAL_ANALYSIS_PROMPT.format(text=text, context=context_str)
        elif analysis_type == "synthesis":
            # –î–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            return SYNTHESIS_PROMPT.format(
                ai_results=user_context.get("ai_results", {}),
                original_text=text[:1000] + "..." if len(text) > 1000 else text,
                metadata=user_context
            )
        elif analysis_type == "multi_ai_synthesis":
            # –î–ª—è –º—É–ª—å—Ç–∏-AI —Å–∏–Ω—Ç–µ–∑–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            return MULTI_AI_SYNTHESIS_PROMPT.format(
                ai_results=user_context.get("ai_results", {}),
                original_text=text[:1000] + "..." if len(text) > 1000 else text,
                metadata=user_context
            )
        else:
            # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤
            return self._build_basic_prompt(text, analysis_type, user_context)
    
    def _build_basic_prompt(self, 
                           text: str, 
                           analysis_type: str,
                           user_context: Optional[Dict[str, Any]] = None) -> str:
        """–ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        
        base_prompt = f"""–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥-–∞–Ω–∞–ª–∏—Ç–∏–∫ —Å –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è.

–ó–ê–î–ê–ß–ê: –ü—Ä–æ–≤–µ–¥–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π {analysis_type} –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

–¢–ï–ö–°–¢ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{text}

–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢:
{user_context if user_context else "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω"}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –ê–ù–ê–õ–ò–ó–£:
1. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–º –∏ –Ω–∞—É—á–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º
2. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–∑–Ω–∞–Ω–Ω—ã–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ç–µ–æ—Ä–∏–∏ –∏ –º–æ–¥–µ–ª–∏
3. –£–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –º–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞
4. –û—Ü–µ–Ω–∏ —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –≤—ã–≤–æ–¥–∞—Ö (0-100%)
5. –û—Ç–º–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê (–≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON):
{{
    "analysis_type": "{analysis_type}",
    "main_findings": {{
        "personality_traits": [],
        "emotional_state": "",
        "cognitive_patterns": [],
        "behavioral_indicators": []
    }},
    "detailed_analysis": {{
        "strengths": [],
        "areas_for_development": [],
        "communication_style": "",
        "decision_making_pattern": ""
    }},
    "psychological_profile": {{
        "big_five_traits": {{
            "openness": 0-100,
            "conscientiousness": 0-100,
            "extraversion": 0-100,
            "agreeableness": 0-100,
            "neuroticism": 0-100
        }},
        "disc_profile": "",
        "dominant_motivators": []
    }},
    "confidence_score": 0-100,
    "methodology": [],
    "limitations": [],
    "recommendations": []
}}

–ù–∞—á–∏–Ω–∞–π –∞–Ω–∞–ª–∏–∑:"""

        return base_prompt
    
    async def stream_analysis(self, 
                             text: str, 
                             analysis_type: str = "psychological") -> AsyncGenerator[str, None]:
        """
        –ü–æ—Ç–æ–∫–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å real-time –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analysis_type: –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞
            
        Yields:
            –§—Ä–∞–≥–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        """
        try:
            prompt = self._get_prompt_for_analysis_type(text, analysis_type)
            
            logger.info("üåä –ù–∞—á–∏–Ω–∞—é –ø–æ—Ç–æ–∫–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑", 
                       text_length=len(text), 
                       analysis_type=analysis_type)
            
            async with self.client.messages.stream(
                model=self.model,
                max_tokens=4000,
                temperature=0.3,
                messages=[
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ]
            ) as stream:
                async for text_chunk in stream.text_stream:
                    yield text_chunk
                    
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", error=str(e))
            yield f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
    
    def _parse_analysis_response(self, response_text: str, analysis_type: str) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ Claude"""
        try:
            import json
            import re
            
            logger.info("üîç –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ Claude", text_length=len(response_text))
            
            # –£–¥–∞–ª–µ–Ω–∏–µ markdown –±–ª–æ–∫–æ–≤
            cleaned_text = re.sub(r'```json\s*', '', response_text)
            cleaned_text = re.sub(r'\s*```', '', cleaned_text)
            cleaned_text = cleaned_text.strip()
            
            # –ü–æ–ø—ã—Ç–∫–∞ 1: –ü–æ–∏—Å–∫ –ø–æ–ª–Ω–æ–≥–æ JSON –æ–±—ä–µ–∫—Ç–∞
            json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            json_matches = re.findall(json_pattern, cleaned_text, re.DOTALL)
            
            for json_str in json_matches:
                try:
                    parsed_data = json.loads(json_str)
                    if isinstance(parsed_data, dict) and len(parsed_data) > 3:
                        logger.info("‚úÖ JSON —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω", keys=list(parsed_data.keys()))
                        return parsed_data
                except json.JSONDecodeError:
                    continue
            
            # –ü–æ–ø—ã—Ç–∫–∞ 2: –ü–æ–∏—Å–∫ JSON —Å –±–æ–ª–µ–µ –≥–∏–±–∫–∏–º regex
            json_match = re.search(r'\{.*\}', cleaned_text, re.DOTALL)
            if json_match:
                try:
                    json_str = json_match.group()
                    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ JSON
                    json_str = self._clean_json_string(json_str)
                    parsed_data = json.loads(json_str)
                    
                    logger.info("‚úÖ JSON –Ω–∞–π–¥–µ–Ω –∏ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω (–ø–æ–ø—ã—Ç–∫–∞ 2)")
                    return parsed_data
                except json.JSONDecodeError as e:
                    logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON (–ø–æ–ø—ã—Ç–∫–∞ 2)", error=str(e))
            
            # –ü–æ–ø—ã—Ç–∫–∞ 3: –ï—Å–ª–∏ JSON –≤–æ–æ–±—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω
            logger.info("üìù JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ —Ç–µ–∫—Å—Ç–∞")
            return self._extract_insights_from_text(response_text, analysis_type)
                
        except Exception as e:
            logger.error("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞", error=str(e))
            return self._create_error_structure(response_text, analysis_type, str(e))
    
    def _clean_json_string(self, json_str: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ JSON —Å—Ç—Ä–æ–∫–∏ –æ—Ç –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –ü–æ–¥—Å—á–µ—Ç —Å–∫–æ–±–æ–∫ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è JSON
        open_braces = 0
        clean_json = ""
        
        for char in json_str:
            clean_json += char
            if char == '{':
                open_braces += 1
            elif char == '}':
                open_braces -= 1
                if open_braces == 0:
                    break
        
        return clean_json
    
    def _validate_analysis_structure(self, data: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∞–Ω–∞–ª–∏–∑–∞"""
        required_keys = ["main_findings", "psychological_profile", "confidence_score"]
        return all(key in data for key in required_keys)
    
    def _create_fallback_structure(self, response_text: str, analysis_type: str, partial_data: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ fallback —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–∏ –Ω–µ–ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        return {
            "analysis_type": analysis_type,
            "hook_summary": "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è –ª–∏—á–Ω–æ—Å—Ç—å —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º–∏",
            "personality_core": {
                "essence": partial_data.get("executive_summary", "–ê–Ω–∞–ª–∏–∑ –≤—ã—è–≤–∏–ª –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ª–∏—á–Ω–æ—Å—Ç–∏"),
                "unique_traits": ["–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–ª–∞–¥ —É–º–∞", "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞", "–¢–≤–æ—Ä—á–µ—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª"],
                "hidden_depths": "–ó–∞ –≤–Ω–µ—à–Ω–∏–º —Ñ–∞—Å–∞–¥–æ–º —Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è –±–æ–≥–∞—Ç—ã–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–∏—Ä"
            },
            "main_findings": partial_data.get("main_findings", {
                "personality_traits": ["–í–¥—É–º—á–∏–≤–æ—Å—Ç—å", "–°–∞–º–æ–∞–Ω–∞–ª–∏–∑", "–°—Ç—Ä–µ–º–ª–µ–Ω–∏–µ –∫ —Ä–æ—Å—Ç—É"],
                "emotional_signature": "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ",
                "thinking_style": "–°–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –≥–ª—É–±–æ–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏"
            }),
            "psychological_profile": partial_data.get("psychological_profile", {}),
            "confidence_score": partial_data.get("confidence_score", 75),
            "raw_response": response_text[:500] + "..." if len(response_text) > 500 else response_text
        }
    
    def _extract_insights_from_text(self, response_text: str, analysis_type: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–∑ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ñ—Ä–∞–∑
        text_lower = response_text.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        traits = []
        if "–∏–Ω—Ç—Ä–æ–≤–µ—Ä—Ç" in text_lower or "–∑–∞–º–∫–Ω—É—Ç" in text_lower:
            traits.append("–ò–Ω—Ç—Ä–æ–≤–µ—Ä—Ç–Ω–æ—Å—Ç—å - –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–∏—Ä —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π")
        if "—ç–∫—Å—Ç—Ä–∞–≤–µ—Ä—Ç" in text_lower or "–æ–±—â–∏—Ç–µ–ª—å–Ω" in text_lower:
            traits.append("–≠–∫—Å—Ç—Ä–∞–≤–µ—Ä—Ç–Ω–æ—Å—Ç—å - —ç–Ω–µ—Ä–≥–∏—è –æ—Ç –æ–±—â–µ–Ω–∏—è —Å –ª—é–¥—å–º–∏")
        if "—Ç–≤–æ—Ä—á–µ—Å" in text_lower or "–∫—Ä–µ–∞—Ç–∏–≤" in text_lower:
            traits.append("–ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å - —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ —Ç–≤–æ—Ä—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é")
        if "–∞–Ω–∞–ª–∏—Ç–∏—á" in text_lower or "–ª–æ–≥–∏—á" in text_lower:
            traits.append("–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–ª–∞–¥ —É–º–∞ - –ª—é–±–æ–≤—å –∫ –≥–ª—É–±–æ–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É")
        if "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω" in text_lower or "—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω" in text_lower:
            traits.append("–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ - –±–æ–≥–∞—Ç—ã–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–∏—Ä")
        
        if not traits:
            traits = ["–£–Ω–∏–∫–∞–ª—å–Ω–∞—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ—Å—Ç—å", "–°–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏", "–°—Ç—Ä–µ–º–ª–µ–Ω–∏–µ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é"]
        
        return {
            "analysis_type": analysis_type,
            "hook_summary": "–ê–Ω–∞–ª–∏–∑ –≤—ã—è–≤–∏–ª –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤–∞—à–µ–π –ª–∏—á–Ω–æ—Å—Ç–∏",
            "personality_core": {
                "essence": "–õ–∏—á–Ω–æ—Å—Ç—å —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å–æ—á–µ—Ç–∞–Ω–∏–µ–º —á–µ—Ä—Ç –∏ –≥–ª—É–±–æ–∫–∏–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –º–∏—Ä–æ–º",
                "unique_traits": traits[:4],
                "hidden_depths": "–ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ—Å—Ç—å –ª–∏—á–Ω–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º–∏"
            },
            "main_findings": {
                "personality_traits": traits,
                "emotional_signature": "–¢–µ–∫—Å—Ç –æ—Ç—Ä–∞–∂–∞–µ—Ç –±–æ–≥–∞—Ç—Å—Ç–≤–æ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–π",
                "thinking_style": "–°–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏ –≥–ª—É–±–æ–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –∂–∏–∑–Ω–µ–Ω–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏–π",
                "behavioral_patterns": ["–í–¥—É–º—á–∏–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—è–º", "–í–Ω–∏–º–∞–Ω–∏–µ –∫ –¥–µ—Ç–∞–ª—è–º"]
            },
            "psychological_profile": {
                "big_five_traits": {
                    "openness": {"score": 70, "description": "–û—Ç–∫—Ä—ã—Ç–æ—Å—Ç—å –∫ –Ω–æ–≤–æ–º—É –æ–ø—ã—Ç—É"},
                    "conscientiousness": {"score": 65, "description": "–û—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"},
                    "extraversion": {"score": 55, "description": "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å"},
                    "agreeableness": {"score": 75, "description": "–î–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –ª—é–¥—è–º"},
                    "neuroticism": {"score": 45, "description": "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å"}
                }
            },
            "practical_insights": {
                "strengths_to_leverage": ["–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –≥–ª—É–±–æ–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É", "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å"],
                "career_alignment": "–ü–æ–¥—Ö–æ–¥—è—Ç —Å—Ñ–µ—Ä—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –≤–¥—É–º—á–∏–≤–æ—Å—Ç–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞",
                "relationship_style": "–¶–µ–Ω–∏—Ç –≥–ª—É–±–æ–∫–∏–µ, –∏—Å–∫—Ä–µ–Ω–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è"
            },
            "actionable_recommendations": {
                "immediate_actions": [
                    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –∞–Ω–∞–ª–∏–∑—É –≤ –≤–∞–∂–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏—è—Ö",
                    "–†–∞–∑–≤–∏–≤–∞–π—Ç–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
                    "–ò—â–∏—Ç–µ –µ–¥–∏–Ω–æ–º—ã—à–ª–µ–Ω–Ω–∏–∫–æ–≤ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—â–µ–Ω–∏—è"
                ]
            },
            "fascinating_details": {
                "hidden_talents": ["–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≤–∏–¥–µ—Ç—å –≥–ª—É–±–∏–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã", "–≠–º–ø–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏"]
            },
            "confidence_score": 75,
            "status": "extracted_from_text",
            "raw_response": response_text[:500] + "..." if len(response_text) > 500 else response_text
        }
    
    def _create_error_structure(self, response_text: str, analysis_type: str, error: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        return {
            "analysis_type": analysis_type,
            "error": error,
            "confidence_score": 30,
            "status": "error",
            "raw_response": response_text[:200] + "..." if len(response_text) > 200 else response_text
        }
    
    async def batch_analyze(self, texts: List[str], analysis_type: str = "psychological") -> List[Dict[str, Any]]:
        """
        –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analysis_type: –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        """
        logger.info("üìä –ù–∞—á–∏–Ω–∞—é –º–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑", count=len(texts))
        
        tasks = []
        for i, text in enumerate(texts):
            task = self.analyze_text(text, analysis_type, {"batch_index": i})
            tasks.append(task)
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
        semaphore = asyncio.Semaphore(3)  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞
        
        async def limited_analyze(task):
            async with semaphore:
                return await task
        
        results = await asyncio.gather(*[limited_analyze(task) for task in tasks])
        
        logger.info("‚úÖ –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω", 
                   completed=len(results),
                   failed=len([r for r in results if "error" in r]))
        
        return results
    
    async def validate_analysis(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ Claude
        
        Args:
            analysis_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
        Returns:
            –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞
        """
        try:
            validation_prompt = f"""–¢—ã - –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∞–Ω–∞–ª–∏–∑–æ–≤.

–ó–ê–î–ê–ß–ê: –û—Ü–µ–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.

–ê–ù–ê–õ–ò–ó –î–õ–Ø –í–ê–õ–ò–î–ê–¶–ò–ò:
{analysis_result}

–ö–†–ò–¢–ï–†–ò–ò –û–¶–ï–ù–ö–ò:
1. –ù–∞—É—á–Ω–∞—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤—ã–≤–æ–¥–æ–≤
2. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º —Ç–µ–æ—Ä–∏—è–º
3. –õ–æ–≥–∏—á–µ—Å–∫–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
4. –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
5. –ê–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç—å —É—Ä–æ–≤–Ω—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏

–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:
{{
    "validation_score": 0-100,
    "strengths": [],
    "weaknesses": [],
    "recommendations": [],
    "revised_confidence": 0-100,
    "validator_notes": ""
}}"""

            message = await self.client.messages.create(
                model=self.model,
                max_tokens=2000,
                temperature=0.1,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
                messages=[{"role": "user", "content": validation_prompt}]
            )
            
            response_text = ""
            for content_block in message.content:
                if content_block.type == "text":
                    response_text += content_block.text
            
            validation_result = self._parse_analysis_response(response_text, "validation")
            
            logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞", 
                       validation_score=validation_result.get('validation_score', 0))
            
            return validation_result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞", error=str(e))
            return {
                "validation_score": 0,
                "error": str(e),
                "status": "validation_failed"
            }
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç–∞ –∏ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if hasattr(self.client, '_client'):
            await self.client._client.aclose()
        logger.info("üîÑ AnthropicClient –∑–∞–∫—Ä—ã—Ç")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∏–µ–Ω—Ç–∞
anthropic_client = AnthropicClient() 