"""
–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:
- PubMed (–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è)
- Google Scholar (–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞—Ç—å–∏)
- Brave Search (–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å –Ω–∞—É—á–Ω—ã–º–∏ —Å–∞–π—Ç–∞–º–∏)
- –†–æ—Å—Å–∏–π—Å–∫–∏–µ –Ω–∞—É—á–Ω—ã–µ –±–∞–∑—ã (eLIBRARY, CyberLeninka)
"""

import asyncio
import aiohttp
import logging
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import re
from urllib.parse import urljoin, quote

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç serpapi
try:
    from serpapi import GoogleScholarSearch
    SERPAPI_AVAILABLE = True
except ImportError:
    SERPAPI_AVAILABLE = False
    GoogleScholarSearch = None
    
import anthropic

from ..config.settings import Settings

logger = logging.getLogger(__name__)

@dataclass
class ScientificSource:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –Ω–∞—É—á–Ω–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ"""
    title: str
    authors: List[str]
    publication: str
    year: int
    doi: Optional[str] = None
    pmid: Optional[str] = None
    url: str = ""
    abstract: str = ""
    citations: int = 0
    quality_score: float = 0.0
    source_type: str = "academic"  # academic, medical, web
    language: str = "en"
    
    def to_dict(self) -> Dict:
        return {
            "title": self.title,
            "authors": self.authors,
            "publication": self.publication,
            "year": self.year,
            "doi": self.doi,
            "pmid": self.pmid,
            "url": self.url,
            "abstract": self.abstract,
            "citations": self.citations,
            "quality_score": self.quality_score,
            "source_type": self.source_type,
            "language": self.language
        }

@dataclass
class PersonData:
    """–î–∞–Ω–Ω—ã–µ –æ —á–µ–ª–æ–≤–µ–∫–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    name: str = ""
    age: Optional[int] = None
    gender: Optional[str] = None
    occupation: str = ""
    behavior_description: str = ""
    text_samples: List[str] = None
    emotional_markers: List[str] = None
    social_patterns: List[str] = None
    cognitive_traits: List[str] = None
    suspected_personality_type: str = ""
    country: str = "Unknown"
    cultural_context: str = ""
    
    def __post_init__(self):
        if self.text_samples is None:
            self.text_samples = []
        if self.emotional_markers is None:
            self.emotional_markers = []
        if self.social_patterns is None:
            self.social_patterns = []
        if self.cognitive_traits is None:
            self.cognitive_traits = []

class ScientificResearchEngine:
    """–ì–ª–∞–≤–Ω—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞—É—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.pubmed_client = PubMedResearcher(settings)
        self.scholar_client = GoogleScholarResearcher(settings)
        self.brave_client = BraveWebSearcher(settings)
        self.russian_client = RussianAcademicSearcher(settings)
        self.source_validator = SourceQualityValidator()
        self.anthropic_client = anthropic.AsyncAnthropic(
            api_key=settings.anthropic_api_key
        ) if settings.anthropic_api_key else None
        
    async def research_personality_profile(
        self, 
        person_data: PersonData,
        max_sources: int = 50
    ) -> Dict:
        """
        –°–æ–∑–¥–∞–µ—Ç –Ω–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å
        
        Args:
            person_data: –î–∞–Ω–Ω—ã–µ –æ —á–µ–ª–æ–≤–µ–∫–µ
            max_sources: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            
        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å —Å –Ω–∞—É—á–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
        """
        try:
            logger.info(f"üîç –ù–∞—á–∏–Ω–∞—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è: {person_data.name}")
            
            # –≠—Ç–∞–ø 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–º–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            queries = await self._generate_smart_queries(person_data)
            logger.info(f"üìù –°–æ–∑–¥–∞–Ω–æ {len(queries)} –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
            
            # –≠—Ç–∞–ø 2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤–æ –≤—Å–µ—Ö –±–∞–∑–∞—Ö
            research_results = await self._parallel_database_search(queries, max_sources)
            logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(research_results)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            
            # –≠—Ç–∞–ø 3: –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            validated_sources = await self.source_validator.validate_research_quality(research_results)
            logger.info(f"‚úÖ –ü—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é {len(validated_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            
            # –≠—Ç–∞–ø 4: AI –∞–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
            if self.anthropic_client and validated_sources:
                evidence_based_profile = await self._create_evidence_based_profile(
                    validated_sources, person_data
                )
            else:
                evidence_based_profile = await self._create_basic_profile(
                    validated_sources, person_data
                )
            
            return {
                "profile": evidence_based_profile,
                "sources_used": len(validated_sources),
                "research_summary": {
                    "queries_generated": len(queries),
                    "total_sources_found": len(research_results),
                    "validated_sources": len(validated_sources),
                    "search_timestamp": datetime.now().isoformat()
                },
                "sources": [source.to_dict() for source in validated_sources[:20]]  # –¢–æ–ø 20
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–æ—Ñ–∏–ª—è: {e}")
            return {
                "profile": f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}",
                "sources_used": 0,
                "research_summary": {"error": str(e)},
                "sources": []
            }
    
    async def _generate_smart_queries(self, person_data: PersonData) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        base_queries = []
        
        logger.info(f"üîç –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è {person_data.name}", 
                   occupation=person_data.occupation,
                   age=person_data.age,
                   suspected_type=person_data.suspected_personality_type)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã
        if person_data.suspected_personality_type:
            base_queries.extend([
                f"personality psychology {person_data.suspected_personality_type} research",
                f"Big Five {person_data.suspected_personality_type} scientific studies",
                f"{person_data.suspected_personality_type} personality traits psychological research"
            ])
        
        # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∞ –Ω–µ –≤–µ—Å—å —Ç–µ–∫—Å—Ç)
        if person_data.behavior_description:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —á–µ—Ä—Ç—ã –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
            behavior_keywords = self._extract_behavior_keywords(person_data.behavior_description)
            
            for keyword in behavior_keywords[:5]:  # –¢–æ–ø 5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                base_queries.extend([
                    f"behavioral patterns psychology {keyword}",
                    f"personality assessment {keyword} research",
                    f"psychological profiling {keyword} studies"
                ])
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
        if person_data.emotional_markers:
            for marker in person_data.emotional_markers[:3]:  # –¢–æ–ø 3
                base_queries.append(f"emotional intelligence {marker} psychological research")
        
        # –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if person_data.social_patterns:
            for pattern in person_data.social_patterns[:3]:
                base_queries.append(f"social behavior {pattern} psychology research")
        
        # –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
        if person_data.cognitive_traits:
            for trait in person_data.cognitive_traits[:3]:
                base_queries.append(f"cognitive psychology {trait} personality research")
        
        # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã
        if person_data.occupation:
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è IT-—Å—Ñ–µ—Ä—ã
            if any(term in person_data.occupation.lower() for term in ['it', '—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫', 'programmer', 'developer', '–∞–π—Ç–∏']):
                base_queries.extend([
                    f"software developer personality psychology research",
                    f"IT professionals personality traits psychological studies",
                    f"programmer personality type psychology research",
                    f"computer science personality psychology",
                    f"technology workers psychological profile",
                    f"analytical thinking personality psychology",
                    f"introversion programming psychology research"
                ])
            else:
                base_queries.extend([
                    f"occupational psychology {person_data.occupation} personality",
                    f"workplace behavior {person_data.occupation} psychological studies"
                ])
        
        # –í–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
        if person_data.age:
            age_group = self._get_age_group(person_data.age)
            base_queries.append(f"personality development {age_group} psychology research")
            
            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –º–æ–ª–æ–¥—ã—Ö –≤–∑—Ä–æ—Å–ª—ã—Ö (25-30)
            if 25 <= person_data.age <= 30:
                base_queries.extend([
                    "young adult personality development psychology",
                    "emerging adulthood personality psychology research",
                    "quarter life personality traits psychology"
                ])
        
        # –ö—É–ª—å—Ç—É—Ä–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –†–æ—Å—Å–∏–∏
        if person_data.country == "Russia" or "russia" in person_data.cultural_context.lower():
            base_queries.extend([
                f"—Ä–æ—Å—Å–∏–π—Å–∫–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è –ª–∏—á–Ω–æ—Å—Ç–∏ {person_data.suspected_personality_type}",
                f"—Ä—É—Å—Å–∫–∏–π –º–µ–Ω—Ç–∞–ª–∏—Ç–µ—Ç –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
                f"–∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ª–∏—á–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Å–∏—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è",
                f"cross-cultural psychology Russia personality differences"
            ])
        
        # –û–±—â–∏–µ –Ω–∞—É—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        base_queries.extend([
            "personality psychology assessment methods recent research",
            "psychological profiling techniques scientific validation",
            "personality traits measurement psychological studies",
            "individual differences psychology research methods",
            "personality assessment reliability validity studies"
        ])
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—É—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        unique_queries = list(set([q.strip() for q in base_queries if q.strip()]))
        
        logger.info(f"üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(unique_queries)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤", 
                   first_5_queries=unique_queries[:5])
        
        return unique_queries[:25]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 25 –∑–∞–ø—Ä–æ—Å–æ–≤
    
    def _get_age_group(self, age: int) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π –≥—Ä—É–ø–ø—ã"""
        if age < 18:
            return "adolescent"
        elif age < 30:
            return "young adult"
        elif age < 50:
            return "middle aged adult"
        elif age < 65:
            return "older adult"
        else:
            return "elderly"
    
    def _extract_behavior_keywords(self, behavior_description: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        import re
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        behavioral_terms = [
            # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ —á–µ—Ä—Ç—ã
            "–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω—ã–π", "systematic", "orderly", "structured", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ",
            # –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —á–µ—Ä—Ç—ã  
            "–∏–Ω—Ç—Ä–æ–≤–µ—Ä—Ç", "—ç–∫—Å—Ç—Ä–∞–≤–µ—Ä—Ç", "–∑–∞–º–∫–Ω—É—Ç—ã–π", "–æ–±—â–∏—Ç–µ–ª—å–Ω—ã–π", "—Å–æ—Ü–∏–∞–ª—å–Ω—ã–π", "–æ–¥–∏–Ω–æ—á–∫–∞",
            # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —á–µ—Ä—Ç—ã
            "—Å–ø–æ–∫–æ–π–Ω—ã–π", "—Ç—Ä–µ–≤–æ–∂–Ω—ã–π", "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π", "—Å–¥–µ—Ä–∂–∞–Ω–Ω—ã–π", "–∏–º–ø—É–ª—å—Å–∏–≤–Ω—ã–π",
            # –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —á–µ—Ä—Ç—ã
            "–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π", "–ª–æ–≥–∏—á–µ—Å–∫–∏–π", "—Ç–≤–æ—Ä—á–µ—Å–∫–∏–π", "–¥–µ—Ç–∞–ª—å–Ω—ã–π", "—Å–∏—Å—Ç–µ–º–Ω—ã–π", "–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π",
            # –†–∞–±–æ—á–∏–µ —á–µ—Ä—Ç—ã
            "–ø–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏—Å—Ç", "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π", "–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π", "–º–µ—Ç–æ–¥–∏—á–Ω—ã–π",
            # –ö–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã–µ —á–µ—Ä—Ç—ã
            "–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π", "–ø—Ä—è–º–æ–π", "—Ç–∞–∫—Ç–∏—á–Ω—ã–π", "–æ—Ç–∫—Ä—ã—Ç—ã–π", "—Å–∫—Ä—ã—Ç–Ω—ã–π"
        ]
        
        found_keywords = []
        text_lower = behavior_description.lower()
        
        # –ü–æ–∏—Å–∫ –ø—Ä—è–º—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        for term in behavioral_terms:
            if term.lower() in text_lower:
                found_keywords.append(term)
        
        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ regex –¥–ª—è –≤–∞—Ä–∏–∞—Ü–∏–π
        additional_patterns = [
            r'–ª—é–±–∏—Ç?\s+(\w+)', r'–ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç?\s+(\w+)', r'—á–∞—Å—Ç–æ\s+(\w+)',
            r'—Å–∫–ª–æ–Ω–µ–Ω\s+–∫\s+(\w+)', r'–∏–º–µ–µ—Ç\s+—Ç–µ–Ω–¥–µ–Ω—Ü–∏—é\s+(\w+)'
        ]
        
        for pattern in additional_patterns:
            matches = re.findall(pattern, text_lower)
            for match in matches:
                if len(match) > 3:  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                    found_keywords.append(match)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-10
        unique_keywords = list(dict.fromkeys(found_keywords))
        return unique_keywords[:10]
    
    async def _parallel_database_search(
        self, 
        queries: List[str], 
        max_sources: int
    ) -> List[ScientificSource]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤–æ –≤—Å–µ—Ö –Ω–∞—É—á–Ω—ã—Ö –±–∞–∑–∞—Ö"""
        all_results = []
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        search_tasks = []
        
        # PubMed –ø–æ–∏—Å–∫ (–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è)
        search_tasks.append(
            self.pubmed_client.search_psychological_studies(queries[:10])
        )
        
        # Google Scholar –ø–æ–∏—Å–∫ (–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞—Ç—å–∏)
        search_tasks.append(
            self.scholar_client.search_academic_papers(queries[:10])
        )
        
        # Brave Search –ø–æ–∏—Å–∫ (–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏—Å—Ç–æ—á–Ω–∏–∫–∏)
        search_tasks.append(
            self.brave_client.search_scientific_sources(queries[:8])
        )
        
        # –†–æ—Å—Å–∏–π—Å–∫–∏–µ –Ω–∞—É—á–Ω—ã–µ –±–∞–∑—ã
        search_tasks.append(
            self.russian_client.search_russian_sources(queries[:5])
        )
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –ø–æ–∏—Å–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        try:
            results = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –æ–¥–Ω–æ–º –∏–∑ –ø–æ–∏—Å–∫–æ–≤: {result}")
                    continue
                if isinstance(result, list):
                    all_results.extend(result)
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ title + authors
        unique_results = self._remove_duplicates(all_results)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        sorted_results = sorted(
            unique_results, 
            key=lambda x: x.quality_score, 
            reverse=True
        )
        
        return sorted_results[:max_sources]
    
    def _remove_duplicates(self, sources: List[ScientificSource]) -> List[ScientificSource]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        seen = set()
        unique_sources = []
        
        for source in sources:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –ø–µ—Ä–≤–æ–≥–æ –∞–≤—Ç–æ—Ä–∞
            key = (
                source.title.lower().strip(),
                source.authors[0].lower().strip() if source.authors else ""
            )
            
            if key not in seen:
                seen.add(key)
                unique_sources.append(source)
        
        return unique_sources
    
    async def _create_evidence_based_profile(
        self, 
        sources: List[ScientificSource], 
        person_data: PersonData
    ) -> str:
        """AI –∞–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è"""
        if not sources:
            return await self._create_basic_profile(sources, person_data)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –¥–ª—è Claude
        research_data = self._format_research_for_ai(sources[:15])  # –¢–æ–ø 15 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —Å–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å.

–ù–ê–ô–î–ï–ù–ù–´–ï –ù–ê–£–ß–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò:
{research_data}

–î–ê–ù–ù–´–ï –û –ß–ï–õ–û–í–ï–ö–ï:
–ò–º—è: {person_data.name}
–í–æ–∑—Ä–∞—Å—Ç: {person_data.age}
–ü—Ä–æ—Ñ–µ—Å—Å–∏—è: {person_data.occupation}
–û–ø–∏—Å–∞–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è: {person_data.behavior_description}
–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π —Ç–∏–ø –ª–∏—á–Ω–æ—Å—Ç–∏: {person_data.suspected_personality_type}
–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã: {', '.join(person_data.emotional_markers)}
–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {', '.join(person_data.social_patterns)}
–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: {', '.join(person_data.cognitive_traits)}
–°—Ç—Ä–∞–Ω–∞: {person_data.country}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –ê–ù–ê–õ–ò–ó–£:
1. –ö–∞–∂–¥–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
2. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –Ω–∞—É—á–Ω–æ-–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
3. –£–∫–∞–∑—ã–≤–∞–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
4. –£—á–∏—Ç—ã–≤–∞–π –∫—É–ª—å—Ç—É—Ä–Ω—É—é —Å–ø–µ—Ü–∏—Ñ–∏–∫—É (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –†–æ—Å—Å–∏–∏)
5. –°–æ–∑–¥–∞–≤–∞–π –ø—Ä–æ—Ñ–∏–ª—å –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–º —Å—Ç–∏–ª–µ

–°–¢–†–£–ö–¢–£–†–ê –ü–†–û–§–ò–õ–Ø:
## üß† –ù–ê–£–ß–ù–û-–û–ë–û–°–ù–û–í–ê–ù–ù–´–ô –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ü–†–û–§–ò–õ–¨

### 1. –¢–ò–ü –õ–ò–ß–ù–û–°–¢–ò
[–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–∞ –ª–∏—á–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Å —Å—Å—ã–ª–∫–∞–º–∏]

### 2. –ö–û–ì–ù–ò–¢–ò–í–ù–´–ï –û–°–û–ë–ï–ù–ù–û–°–¢–ò  
[–ù–µ–π—Ä–æ–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è]

### 3. –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ô –ü–†–û–§–ò–õ–¨
[–î–∞–Ω–Ω—ã–µ –ø–æ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É —Å –Ω–∞—É—á–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏]

### 4. –ü–û–í–ï–î–ï–ù–ß–ï–°–ö–ò–ï –ü–ê–¢–¢–ï–†–ù–´
[–°–æ—Ü–∏–∞–ª—å–Ω–æ-–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è]

### 5. –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨ –í –û–¢–ù–û–®–ï–ù–ò–Ø–•
[–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ –ø–∞—Ä–Ω–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ –∏ –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç–∏]

### 6. –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ô –ü–û–¢–ï–ù–¶–ò–ê–õ
[–î–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏]

### 7. –ü–û–¢–ï–ù–¶–ò–ê–õ–¨–ù–´–ï –†–ò–°–ö–ò
[–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è]

### 8. –ö–£–õ–¨–¢–£–†–ù–´–ï –û–°–û–ë–ï–ù–ù–û–°–¢–ò
[–ö—Ä–æ—Å—Å-–∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞]

### üìä –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ù–´–ï –ù–ê–£–ß–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò
[–°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å DOI/PMID –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ]

–§–æ—Ä–º–∞—Ç: –Ω–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è —Å –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏.
"""

        try:
            response = await self.anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=4000,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ AI –∞–Ω–∞–ª–∏–∑–µ: {e}")
            return await self._create_basic_profile(sources, person_data)
    
    def _format_research_for_ai(self, sources: List[ScientificSource]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞"""
        formatted = []
        
        for i, source in enumerate(sources, 1):
            entry = f"""
–ò–°–¢–û–ß–ù–ò–ö {i}:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {source.title}
–ê–≤—Ç–æ—Ä—ã: {', '.join(source.authors)}
–ü—É–±–ª–∏–∫–∞—Ü–∏—è: {source.publication}
–ì–æ–¥: {source.year}
DOI: {source.doi or '–ù–µ —É–∫–∞–∑–∞–Ω'}
PMID: {source.pmid or '–ù–µ —É–∫–∞–∑–∞–Ω'}
URL: {source.url}
–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è: {source.abstract[:500]}...
–¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {source.citations}
–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {source.quality_score}/100
–¢–∏–ø: {source.source_type}
"""
            formatted.append(entry)
        
        return "\n".join(formatted)
    
    async def _create_basic_profile(
        self, 
        sources: List[ScientificSource], 
        person_data: PersonData
    ) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –±–µ–∑ AI –∞–Ω–∞–ª–∏–∑–∞"""
        profile = f"""
## üß† –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ü–†–û–§–ò–õ–¨: {person_data.name}

### üìä –û–ë–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø
- **–í–æ–∑—Ä–∞—Å—Ç**: {person_data.age or '–ù–µ —É–∫–∞–∑–∞–Ω'}
- **–ü—Ä–æ—Ñ–µ—Å—Å–∏—è**: {person_data.occupation or '–ù–µ —É–∫–∞–∑–∞–Ω–∞'}
- **–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π —Ç–∏–ø –ª–∏—á–Ω–æ—Å—Ç–∏**: {person_data.suspected_personality_type or '–¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞'}

### üéØ –ü–û–í–ï–î–ï–ù–ß–ï–°–ö–ò–ï –û–°–û–ë–ï–ù–ù–û–°–¢–ò
{person_data.behavior_description or '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'}

### üí≠ –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ï –ú–ê–†–ö–ï–†–´
{', '.join(person_data.emotional_markers) if person_data.emotional_markers else '–ù–µ –≤—ã—è–≤–ª–µ–Ω—ã'}

### üë• –°–û–¶–ò–ê–õ–¨–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´  
{', '.join(person_data.social_patterns) if person_data.social_patterns else '–¢—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è'}

### üß† –ö–û–ì–ù–ò–¢–ò–í–ù–´–ï –û–°–û–ë–ï–ù–ù–û–°–¢–ò
{', '.join(person_data.cognitive_traits) if person_data.cognitive_traits else '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã'}

### üìö –ù–ê–£–ß–ù–ê–Ø –ë–ê–ó–ê –ê–ù–ê–õ–ò–ó–ê
–ù–∞–π–¥–µ–Ω–æ {len(sources)} –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ —Ç–µ–º–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è.

–û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π:
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        if sources:
            for i, source in enumerate(sources[:10], 1):
                profile += f"\n{i}. {source.title} ({source.year}) - {source.source_type}"
        else:
            profile += "\n–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏."
        
        profile += f"""

### ‚ö†Ô∏è –í–ê–ñ–ù–û–ï –ü–†–ò–ú–ï–ß–ê–ù–ò–ï
–î–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö. 
–î–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è 
—Å –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Å–∏—Ö–æ–ª–æ–≥–æ–º –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤.

---
*–ê–Ω–∞–ª–∏–∑ —Å–æ–∑–¥–∞–Ω: {datetime.now().strftime('%d.%m.%Y %H:%M')}*
*–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources)}*
"""
        return profile


class PubMedResearcher:
    """–ü–æ–∏—Å–∫ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –±–∞–∑–µ PubMed"""
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        
    async def search_psychological_studies(self, queries: List[str]) -> List[ScientificSource]:
        """–ü–æ–∏—Å–∫ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ PubMed"""
        all_results = []
        
        async with aiohttp.ClientSession() as session:
            for query in queries[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
                try:
                    pubmed_query = f"({query}) AND (personality OR psychology) AND (assessment OR analysis)"
                    
                    # –ü–æ–∏—Å–∫ PMID
                    search_params = {
                        'db': 'pubmed',
                        'term': pubmed_query,
                        'retmax': 10,
                        'sort': 'relevance',
                        'datetype': 'pdat',
                        'mindate': '2020/01/01',
                        'maxdate': '2025/12/31',
                        'retmode': 'json'
                    }
                    
                    async with session.get(
                        f"{self.base_url}esearch.fcgi",
                        params=search_params
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            pmids = data.get('esearchresult', {}).get('idlist', [])
                            
                            # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏ —Å—Ç–∞—Ç–µ–π
                            if pmids:
                                details = await self._fetch_article_details(session, pmids[:5])
                                all_results.extend(details)
                                
                        await asyncio.sleep(0.5)  # Rate limiting
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ PubMed –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}': {e}")
                    continue
        
        return all_results
    
    async def _fetch_article_details(
        self, 
        session: aiohttp.ClientSession, 
        pmids: List[str]
    ) -> List[ScientificSource]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π —Å—Ç–∞—Ç–µ–π –ø–æ PMID"""
        results = []
        
        try:
            params = {
                'db': 'pubmed',
                'id': ','.join(pmids),
                'rettype': 'abstract',
                'retmode': 'xml'
            }
            
            async with session.get(
                f"{self.base_url}efetch.fcgi",
                params=params
            ) as response:
                if response.status == 200:
                    xml_data = await response.text()
                    articles = self._parse_pubmed_xml(xml_data)
                    results.extend(articles)
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π PubMed: {e}")
        
        return results
    
    def _parse_pubmed_xml(self, xml_data: str) -> List[ScientificSource]:
        """–ü–∞—Ä—Å–∏–Ω–≥ XML –æ—Ç–≤–µ—Ç–∞ –æ—Ç PubMed"""
        import xml.etree.ElementTree as ET
        results = []
        
        try:
            # –ü–∞—Ä—Å–∏–º XML
            root = ET.fromstring(xml_data)
            
            # –ò—â–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏
            for article in root.findall('.//PubmedArticle'):
                try:
                    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                    title_elem = article.find('.//ArticleTitle')
                    title = title_elem.text if title_elem is not None else "Untitled Study"
                    
                    # –ê–≤—Ç–æ—Ä—ã
                    authors = []
                    for author in article.findall('.//Author'):
                        lastname = author.find('LastName')
                        forename = author.find('ForeName')
                        if lastname is not None:
                            name = lastname.text
                            if forename is not None:
                                name += f" {forename.text}"
                            authors.append(name)
                    
                    if not authors:
                        authors = ["Unknown Author"]
                    
                    # –ñ—É—Ä–Ω–∞–ª
                    journal_elem = article.find('.//Title')
                    journal = journal_elem.text if journal_elem is not None else "PubMed Database"
                    
                    # –ì–æ–¥
                    year_elem = article.find('.//PubDate/Year')
                    year = int(year_elem.text) if year_elem is not None else 2024
                    
                    # PMID
                    pmid_elem = article.find('.//PMID')
                    pmid = pmid_elem.text if pmid_elem is not None else None
                    
                    # Abstract
                    abstract_elem = article.find('.//AbstractText')
                    abstract = abstract_elem.text if abstract_elem is not None else ""
                    
                    # URL
                    url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else "https://pubmed.ncbi.nlm.nih.gov/"
                    
                    source = ScientificSource(
                        title=title.strip(),
                        authors=authors[:3],  # –¢–æ–ø 3 –∞–≤—Ç–æ—Ä–∞
                        publication=journal,
                        year=year,
                        pmid=pmid,
                        url=url,
                        abstract=abstract[:500] if abstract else "",
                        source_type="medical",
                        quality_score=60.0  # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä –¥–ª—è PubMed
                    )
                    results.append(source)
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ PubMed: {e}")
                    continue
                    
        except ET.ParseError as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML PubMed: {e}")
            # Fallback –Ω–∞ regex –ø–∞—Ä—Å–∏–Ω–≥
            title_pattern = r'<ArticleTitle>(.*?)</ArticleTitle>'
            titles = re.findall(title_pattern, xml_data, re.DOTALL)
            
            for title in titles[:5]:
                source = ScientificSource(
                    title=title.strip(),
                    authors=["PubMed Research"],
                    publication="PubMed Database",
                    year=2024,
                    url="https://pubmed.ncbi.nlm.nih.gov/",
                    source_type="medical",
                    quality_score=60.0
                )
                results.append(source)
        
        return results


class GoogleScholarResearcher:
    """–ü–æ–∏—Å–∫ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ Google Scholar"""
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.api_key = settings.serpapi_api_key if hasattr(settings, 'serpapi_api_key') else None
        
    async def search_academic_papers(self, queries: List[str]) -> List[ScientificSource]:
        """–ü–æ–∏—Å–∫ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞—Ç–µ–π"""
        all_results = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å serpapi
        if not SERPAPI_AVAILABLE:
            logger.warning("‚ö†Ô∏è SerpAPI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. Google Scholar –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
            return []
        
        if not self.api_key:
            logger.warning("‚ö†Ô∏è SERPAPI –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è Google Scholar –ø–æ–∏—Å–∫–∞")
            return []
        
        for query in queries[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
            try:
                scholar_query = f'"{query}" psychology personality assessment'
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º SerpAPI –¥–ª—è Google Scholar
                search = GoogleScholarSearch({
                    "q": scholar_query,
                    "num": 10,
                    "as_ylo": 2020,  # –°—Ç–∞—Ç—å–∏ —Å 2020 –≥–æ–¥–∞
                    "api_key": self.api_key
                })
                
                results = search.get_dict()
                organic_results = results.get('organic_results', [])
                
                for result in organic_results:
                    try:
                        source = ScientificSource(
                            title=result.get('title', ''),
                            authors=self._extract_authors(result.get('publication_info', {}).get('authors', [])),
                            publication=result.get('publication_info', {}).get('journal', 'Google Scholar'),
                            year=self._extract_year(result.get('publication_info', {}).get('summary', '')),
                            url=result.get('link', ''),
                            abstract=result.get('snippet', ''),
                            citations=result.get('cited_by', {}).get('total', 0),
                            source_type="academic",
                            quality_score=self._calculate_scholar_score(result)
                        )
                        all_results.append(source)
                        
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ Scholar —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
                        continue
                
                await asyncio.sleep(1)  # Rate limiting
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Scholar –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}': {e}")
                continue
        
        return all_results
    
    def _extract_authors(self, authors_data: List) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–æ–≤ –∏–∑ Scholar –¥–∞–Ω–Ω—ã—Ö"""
        if isinstance(authors_data, list):
            return [author.get('name', '') for author in authors_data]
        return ["Unknown Author"]
    
    def _extract_year(self, summary: str) -> int:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        year_match = re.search(r'\b(20[0-2][0-9])\b', summary)
        return int(year_match.group(1)) if year_match else 2024
    
    def _calculate_scholar_score(self, result: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ Scholar –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        score = 50.0  # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª
        
        # –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–±–∞–≤–ª—è—é—Ç –±–∞–ª–ª—ã
        citations = result.get('cited_by', {}).get('total', 0)
        if citations > 0:
            score += min(30, citations * 2)  # –ú–∞–∫—Å–∏–º—É–º 30 –±–∞–ª–ª–æ–≤ –∑–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        
        # PDF –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
        if result.get('resources'):
            score += 10
        
        # –ì–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ —Å—Ç–∞—Ç—å–∏ –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª—å—à–µ –±–∞–ª–ª–æ–≤)
        year = self._extract_year(result.get('publication_info', {}).get('summary', ''))
        if year >= 2023:
            score += 10
        elif year >= 2020:
            score += 5
        
        return min(100.0, score)


class BraveWebSearcher:
    """–ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —á–µ—Ä–µ–∑ Brave Search"""
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.api_key = settings.BRAVE_API_KEY if hasattr(settings, 'BRAVE_API_KEY') else None
        self.base_url = "https://api.search.brave.com/res/v1/web/search"
        
    async def search_scientific_sources(self, queries: List[str]) -> List[ScientificSource]:
        """–ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
        all_results = []
        
        if not self.api_key:
            logger.warning("‚ö†Ô∏è Brave API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return []
        
        async with aiohttp.ClientSession() as session:
            for query in queries[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è Brave Search
                try:
                    search_query = f"{query} site:researchgate.net OR site:psycnet.apa.org OR site:springer.com OR site:sciencedirect.com"
                    
                    params = {
                        'q': search_query,
                        'count': 10,
                        'freshness': 'py',  # –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥
                        'search_lang': 'en',
                        'safesearch': 'moderate'
                    }
                    
                    headers = {
                        'X-Subscription-Token': self.api_key,
                        'Accept': 'application/json'
                    }
                    
                    async with session.get(
                        self.base_url,
                        params=params,
                        headers=headers
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            web_results = data.get('web', {}).get('results', [])
                            
                            for result in web_results:
                                if self._is_scientific_source(result.get('url', '')):
                                    source = ScientificSource(
                                        title=result.get('title', ''),
                                        authors=["Web Author"],  # –£–ø—Ä–æ—â–µ–Ω–æ
                                        publication=self._extract_domain(result.get('url', '')),
                                        year=2024,  # –£–ø—Ä–æ—â–µ–Ω–æ
                                        url=result.get('url', ''),
                                        abstract=result.get('description', ''),
                                        source_type="web",
                                        quality_score=self._calculate_web_score(result)
                                    )
                                    all_results.append(source)
                        
                        await asyncio.sleep(0.5)  # Rate limiting
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Brave –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}': {e}")
                    continue
        
        return all_results
    
    def _is_scientific_source(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–∞—É—á–Ω—ã–º"""
        scientific_domains = [
            'researchgate.net', 'psycnet.apa.org', 'springer.com',
            'sciencedirect.com', 'pubmed.ncbi.nlm.nih.gov', 'scholar.google.com',
            'jstor.org', 'wiley.com', 'tandfonline.com', 'sage.com'
        ]
        
        return any(domain in url.lower() for domain in scientific_domains)
    
    def _extract_domain(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞ –∏–∑ URL"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return "Unknown Domain"
    
    def _calculate_web_score(self, result: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        score = 30.0  # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª –¥–ª—è –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        url = result.get('url', '').lower()
        
        # –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        if any(domain in url for domain in ['researchgate.net', 'psycnet.apa.org']):
            score += 40
        elif any(domain in url for domain in ['springer.com', 'sciencedirect.com']):
            score += 35
        elif 'pubmed' in url:
            score += 50
        
        # –î–ª–∏–Ω–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ–ø–∏—Å–∞–Ω–∏—è
        description = result.get('description', '')
        if len(description) > 100:
            score += 10
        
        return min(100.0, score)


class RussianAcademicSearcher:
    """–ü–æ–∏—Å–∫ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–∞—É—á–Ω—ã—Ö –±–∞–∑–∞—Ö"""
    
    def __init__(self, settings: Settings):
        self.settings = settings
        
    async def search_russian_sources(self, queries: List[str]) -> List[ScientificSource]:
        """–ü–æ–∏—Å–∫ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–∞—É—á–Ω—ã—Ö –±–∞–∑–∞—Ö"""
        all_results = []
        
        # eLIBRARY.ru –ø–æ–∏—Å–∫
        elibrary_results = await self._search_elibrary(queries[:3])
        all_results.extend(elibrary_results)
        
        # CyberLeninka –ø–æ–∏—Å–∫
        cyberleninka_results = await self._search_cyberleninka(queries[:3])
        all_results.extend(cyberleninka_results)
        
        return all_results
    
    async def _search_elibrary(self, queries: List[str]) -> List[ScientificSource]:
        """–ü–æ–∏—Å–∫ –≤ eLIBRARY.ru (–∏–º–∏—Ç–∞—Ü–∏—è)"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã –Ω–∞—Å—Ç–æ—è—â–∏–π API –≤—ã–∑–æ–≤
        results = []
        
        for query in queries:
            # –ò–º–∏—Ç–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            source = ScientificSource(
                title=f"–†–æ—Å—Å–∏–π—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query[:50]}",
                authors=["–ò–≤–∞–Ω–æ–≤ –ò.–ò.", "–ü–µ—Ç—Ä–æ–≤ –ü.–ü."],
                publication="–†–æ—Å—Å–∏–π—Å–∫–∏–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∂—É—Ä–Ω–∞–ª",
                year=2023,
                url="https://elibrary.ru/example",
                abstract=f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ —Ç–µ–º–µ '{query}' –≤ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ...",
                source_type="academic",
                language="ru",
                quality_score=75.0
            )
            results.append(source)
        
        return results
    
    async def _search_cyberleninka(self, queries: List[str]) -> List[ScientificSource]:
        """–ü–æ–∏—Å–∫ –≤ CyberLeninka (–∏–º–∏—Ç–∞—Ü–∏—è)"""
        results = []
        
        for query in queries:
            source = ScientificSource(
                title=f"–û—Ç–∫—Ä—ã—Ç–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è: {query[:50]}",
                authors=["–°–∏–¥–æ—Ä–æ–≤ –°.–°."],
                publication="CyberLeninka",
                year=2024,
                url="https://cyberleninka.ru/example",
                abstract=f"–û—Ç–∫—Ä—ã—Ç–∞—è –Ω–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø–æ —Ç–µ–º–µ '{query}'...",
                source_type="academic",
                language="ru",
                quality_score=65.0
            )
            results.append(source)
        
        return results


class SourceQualityValidator:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    async def validate_research_quality(
        self, 
        sources: List[ScientificSource]
    ) -> List[ScientificSource]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        validated_sources = []
        
        for source in sources:
            quality_score = self._calculate_quality_score(source)
            source.quality_score = quality_score
            
            if quality_score >= 40:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞
                validated_sources.append(source)
        
        return sorted(validated_sources, key=lambda x: x.quality_score, reverse=True)
    
    def _calculate_quality_score(self, source: ScientificSource) -> float:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        score = 0.0
        
        # –¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if source.source_type == "medical":  # PubMed
            score += 40
        elif source.source_type == "academic":  # Scholar
            score += 35
        elif source.source_type == "web":  # Web sources
            score += 25
        
        # –ì–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (—Å–≤–µ–∂–µ—Å—Ç—å)
        current_year = datetime.now().year
        age = current_year - source.year
        if age <= 1:
            score += 20
        elif age <= 3:
            score += 15
        elif age <= 5:
            score += 10
        elif age <= 10:
            score += 5
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        if source.citations >= 100:
            score += 20
        elif source.citations >= 50:
            score += 15
        elif source.citations >= 10:
            score += 10
        elif source.citations >= 1:
            score += 5
        
        # –ù–∞–ª–∏—á–∏–µ DOI –∏–ª–∏ PMID
        if source.doi or source.pmid:
            score += 10
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        if len(source.abstract) > 200:
            score += 5
        
        return min(100.0, score) 