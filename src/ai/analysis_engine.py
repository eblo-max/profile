"""
–û—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
–ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –≤—Å–µ—Ö AI —Å–µ—Ä–≤–∏—Å–æ–≤ –∏ —Å–æ–∑–¥–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç

üî¨ –ù–û–í–û–ï: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π –Ω–∞—É—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (2025)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ PubMed, Google Scholar
- –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π —Å peer-reviewed –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
- –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—ã–≤–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π AI –∞–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã
"""
import asyncio
import structlog
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict

from src.ai.anthropic_client import anthropic_client
from src.ai.watson_client import OpenAIClient
from src.ai.google_client import google_gemini_client
from src.ai.cohere_client import cohere_client
from src.ai.huggingface_client import huggingface_client
from src.ai.scientific_research_engine import ScientificResearchEngine, PersonData
from src.ai.multi_ai_research_analyzer import MultiAIResearchAnalyzer
from src.database.connection import get_async_session
from src.database.models import Analysis, AnalysisError
from src.config.settings import settings
from src.utils.economic_analysis_manager import economic_manager, AnalysisLevel, CostEstimate
from src.utils.cache_manager import cache_manager

logger = structlog.get_logger()


@dataclass
class AnalysisInput:
    """–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    user_id: int
    telegram_id: int
    text: Optional[str] = None
    images: Optional[List[str]] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class AnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    analysis_id: int
    status: str
    confidence_score: float
    main_findings: Dict[str, Any]
    detailed_analysis: Dict[str, Any]
    psychological_profile: Dict[str, Any]
    final_report: str
    methodology: List[str]
    limitations: List[str]
    bias_warnings: List[str]
    created_at: datetime


class AnalysisEngine:
    """–û—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞"""
        self.claude_client = anthropic_client
        self.openai_client = OpenAIClient()
        self.google_gemini_client = google_gemini_client
        self.cohere_client = cohere_client
        self.huggingface_client = huggingface_client
        
        # üî¨ –ù–û–í–´–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´: –°–∏—Å—Ç–µ–º–∞ –Ω–∞—É—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (2025)
        self.research_engine = ScientificResearchEngine(settings)
        self.multi_ai_analyzer = MultiAIResearchAnalyzer(settings)
        
        # üí∞ –≠–ö–û–ù–û–ú–ò–ß–ï–°–ö–ò–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´ (2025)
        self.economic_manager = economic_manager
        self.cache_manager = cache_manager
        
        self.supported_services = {
            # üöÄ –°–û–í–†–ï–ú–ï–ù–ù–´–ï AI –°–ï–†–í–ò–°–´ (2025)
            "claude": True,  # –ì–ª–∞–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ —Å–∏–Ω—Ç–µ–∑
            "openai": self.openai_client.is_available,  # GPT-4o
            "google_gemini": google_gemini_client.is_available,  # –ó–∞–º–µ–Ω–∞ Google Cloud NL + Azure
            "cohere": cohere_client.is_available,  # –ó–∞–º–µ–Ω–∞ Lexalytics + Receptiviti
            "huggingface": huggingface_client.is_available,  # –ó–∞–º–µ–Ω–∞ AWS Rekognition
            
            # üî¨ –ù–ê–£–ß–ù–´–ô –ü–û–ò–°–ö (2025)
            "scientific_research": True,  # –í—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω
            "multi_ai_research": True,  # –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            
            # üìâ DEPRECATED –°–ï–†–í–ò–°–´ (–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            "azure": settings.azure_cognitive_key is not None,
            "google": settings.google_cloud_project_id is not None,
            "aws": settings.aws_access_key_id is not None,
            "crystal": settings.crystal_api_key is not None,
            "receptiviti": settings.receptiviti_api_key is not None,
            "lexalytics": settings.lexalytics_api_key is not None,
            "monkeylearn": settings.monkeylearn_api_key is not None
        }
        
        active_services = [name for name, active in self.supported_services.items() if active]
        logger.info("üöÄ AnalysisEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –Ω–∞—É—á–Ω—ã–º –ø–æ–∏—Å–∫–æ–º", 
                   active_services=active_services,
                   total_services=len(active_services),
                   scientific_research_enabled=True)
    
    async def analyze_comprehensive(self, analysis_input: AnalysisInput) -> AnalysisResult:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
        
        Args:
            analysis_input: –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ë–î
        analysis_id = await self._create_analysis_record(analysis_input)
        
        try:
            logger.info("üîç –ù–∞—á–∏–Ω–∞—é –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑", 
                       analysis_id=analysis_id,
                       user_id=analysis_input.user_id)
            
            # –≠—Ç–∞–ø 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ—Ç –≤—Å–µ—Ö AI —Å–µ—Ä–≤–∏—Å–æ–≤
            ai_results = await self._collect_ai_insights(analysis_input, analysis_id)
            
            # –≠—Ç–∞–ø 2: –°–∏–Ω—Ç–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ Claude
            synthesis_result = await self._synthesize_results(ai_results, analysis_input)
            
            # –≠—Ç–∞–ø 3: –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            validation_result = await self._validate_analysis(synthesis_result)
            
            # –≠—Ç–∞–ø 4: –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
            final_report = await self._generate_final_report(
                synthesis_result, 
                validation_result, 
                ai_results
            )
            
            # –≠—Ç–∞–ø 5: –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            confidence_score = self._calculate_confidence_score(ai_results, validation_result)
            bias_warnings = self._detect_potential_bias(synthesis_result, ai_results)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = AnalysisResult(
                analysis_id=analysis_id,
                status="completed",
                confidence_score=confidence_score,
                main_findings=synthesis_result.get("main_findings", {}),
                detailed_analysis=synthesis_result.get("detailed_analysis", {}),
                psychological_profile=synthesis_result.get("psychological_profile", {}),
                final_report=final_report,
                methodology=self._get_methodology_used(ai_results),
                limitations=synthesis_result.get("limitations", []),
                bias_warnings=bias_warnings,
                created_at=datetime.utcnow()
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –ë–î
            await self._save_analysis_result(analysis_id, result, ai_results, synthesis_result)
            
            logger.info("‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω", 
                       analysis_id=analysis_id,
                       confidence_score=confidence_score,
                       ai_services_used=len(ai_results))
            
            return result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", 
                        analysis_id=analysis_id, 
                        error=str(e), 
                        exc_info=True)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –≤ –ë–î
            await self._save_analysis_error(analysis_id, "comprehensive_analysis", str(e))
            
            # –í–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –æ—à–∏–±–∫–æ–π
            return AnalysisResult(
                analysis_id=analysis_id,
                status="failed",
                confidence_score=0.0,
                main_findings={"error": str(e)},
                detailed_analysis={},
                psychological_profile={},
                final_report=f"–ê–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è: {str(e)}",
                methodology=[],
                limitations=["–ê–Ω–∞–ª–∏–∑ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏"],
                bias_warnings=[],
                created_at=datetime.utcnow()
            )
    
    async def economic_analysis(
        self, 
        text: str, 
        user_id: int, 
        telegram_id: int,
        level: AnalysisLevel = AnalysisLevel.FREE,
        force_refresh: bool = False
    ) -> Dict[str, Any]:
        """
        üí∞ –≠–ö–û–ù–û–ú–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –° –ö–û–ù–¢–†–û–õ–ï–ú –°–¢–û–ò–ú–û–°–¢–ò
        –£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Ä–∞—Å—Ö–æ–¥–æ–≤
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            telegram_id: Telegram ID
            level: –£—Ä–æ–≤–µ–Ω—å –∞–Ω–∞–ª–∏–∑–∞ (FREE/BASIC/ADVANCED/RESEARCH/PREMIUM)
            force_refresh: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–µ–∑ –∫—ç—à–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        start_time = datetime.utcnow()
        
        try:
            logger.info("üí∞ –ù–∞—á–∏–Ω–∞—é —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑",
                       user_id=user_id,
                       level=level.value,
                       text_length=len(text),
                       force_refresh=force_refresh)
            
            # –≠—Ç–∞–ø 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            limits_ok, limit_reason = await self.economic_manager.check_user_limits(user_id, level)
            if not limits_ok:
                return {
                    "status": "limit_exceeded",
                    "error": limit_reason,
                    "level": level.value,
                    "suggestions": await self._get_upgrade_suggestions(level)
                }
            
            # –≠—Ç–∞–ø 2: –û—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
            cost_estimate = await self.economic_manager.estimate_analysis_cost(level, text, user_id)
            
            # –≠—Ç–∞–ø 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ (–µ—Å–ª–∏ –Ω–µ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ)
            cached_result = None
            if not force_refresh:
                cached_result = await self.cache_manager.get_cached_analysis(
                    text, level.value, user_id
                )
                
                if cached_result:
                    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    cached_result["metadata"]["served_from_cache"] = True
                    cached_result["metadata"]["cost_saved_usd"] = cost_estimate.estimated_cost_usd
                    cached_result["metadata"]["analysis_level"] = level.value
                    
                    logger.info("üéØ –û—Ç–¥–∞–Ω –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
                               user_id=user_id,
                               level=level.value,
                               cost_saved_usd=cost_estimate.estimated_cost_usd)
                    
                    return cached_result
            
            # –≠—Ç–∞–ø 4: –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É —É—Ä–æ–≤–Ω—é
            analysis_result = await self._execute_level_analysis(text, level, user_id, cost_estimate)
            
            # –≠—Ç–∞–ø 5: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏
            actual_cost = analysis_result.get("metadata", {}).get("actual_cost_usd", cost_estimate.estimated_cost_usd)
            tokens_used = analysis_result.get("metadata", {}).get("tokens_used", cost_estimate.estimated_tokens)
            ai_services_used = analysis_result.get("metadata", {}).get("ai_services_used", [])
            
            await self.economic_manager.log_analysis_cost(
                user_id, level, actual_cost, tokens_used, ai_services_used
            )
            
            # –≠—Ç–∞–ø 6: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ)
            if analysis_result.get("status") == "success" and level != AnalysisLevel.PREMIUM:
                await self.cache_manager.cache_analysis_result(
                    text, level.value, analysis_result, user_id
                )
            
            # –≠—Ç–∞–ø 7: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            analysis_result["metadata"].update({
                "execution_time_seconds": execution_time,
                "estimated_cost_usd": cost_estimate.estimated_cost_usd,
                "cache_hit": False,
                "analysis_timestamp": start_time.isoformat()
            })
            
            logger.info("‚úÖ –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω",
                       user_id=user_id,
                       level=level.value,
                       actual_cost_usd=actual_cost,
                       execution_time_seconds=execution_time,
                       status=analysis_result.get("status"))
            
            return analysis_result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
                        user_id=user_id,
                        level=level.value,
                        error=str(e),
                        exc_info=True)
            
            return {
                "status": "error",
                "error": str(e),
                "level": level.value,
                "metadata": {
                    "execution_time_seconds": (datetime.utcnow() - start_time).total_seconds(),
                    "cost_estimate": asdict(cost_estimate) if 'cost_estimate' in locals() else None
                }
            }
    
    async def _execute_level_analysis(
        self,
        text: str,
        level: AnalysisLevel,
        user_id: int,
        cost_estimate: CostEstimate
    ) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É —É—Ä–æ–≤–Ω—é"""
        
        config = self.economic_manager.analysis_configs[level]
        ai_services_used = []
        total_tokens = 0
        actual_cost = 0.0
        
        try:
            if level == AnalysisLevel.FREE:
                # FREE: –¢–æ–ª—å–∫–æ Claude, –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
                result = await self.claude_client.analyze_text(
                    text, "psychological", {"user_id": user_id}
                )
                ai_services_used = ["claude"]
                total_tokens = result.get("tokens_used", 2500)
                actual_cost = 0.0
                
                analysis_content = self._format_free_analysis(result, text)
                
            elif level == AnalysisLevel.BASIC:
                # BASIC: Claude —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
                result = await self.claude_client.analyze_text(
                    text, "comprehensive_psychological", {"user_id": user_id}
                )
                ai_services_used = ["claude"]
                total_tokens = result.get("tokens_used", 4000)
                actual_cost = settings.basic_price_usd
                
                analysis_content = self._format_basic_analysis(result, text)
                
            elif level == AnalysisLevel.ADVANCED:
                # ADVANCED: Claude + OpenAI + –Ω–∞—É—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
                claude_task = self.claude_client.analyze_text(
                    text, "comprehensive_psychological", {"user_id": user_id}
                )
                openai_task = self.openai_client.analyze_psychological_text(text)
                scientific_task = self._get_scientific_sample(text, max_sources=10)
                
                claude_result, openai_result, scientific_result = await asyncio.gather(
                    claude_task, openai_task, scientific_task
                )
                
                ai_services_used = ["claude", "openai"]
                total_tokens = (
                    claude_result.get("tokens_used", 4000) + 
                    openai_result.get("tokens_used", 4000)
                )
                actual_cost = settings.advanced_price_usd
                
                analysis_content = self._format_advanced_analysis(
                    claude_result, openai_result, scientific_result, text
                )
                
            elif level == AnalysisLevel.RESEARCH:
                # RESEARCH: –ü–æ–ª–Ω—ã–π –Ω–∞—É—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                analysis_content = await self.scientific_research_analysis(
                    {"text": text}, user_id, user_id  # telegram_id = user_id –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
                )
                ai_services_used = ["claude", "openai", "gemini", "scientific_research"]
                total_tokens = 15000
                actual_cost = settings.research_price_usd
                
            elif level == AnalysisLevel.PREMIUM:
                # PREMIUM: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                analysis_content = await self._execute_premium_analysis(text, user_id)
                ai_services_used = ["claude", "openai", "gemini", "cohere", "huggingface"]
                total_tokens = 25000
                actual_cost = settings.premium_price_usd
            
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —É—Ä–æ–≤–µ–Ω—å –∞–Ω–∞–ª–∏–∑–∞: {level}")
            
            return {
                "status": "success",
                "analysis": analysis_content,
                "level": level.value,
                "metadata": {
                    "ai_services_used": ai_services_used,
                    "tokens_used": total_tokens,
                    "actual_cost_usd": actual_cost,
                    "sources_count": len(ai_services_used),
                    "served_from_cache": False
                }
            }
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ —É—Ä–æ–≤–Ω—è",
                        level=level.value,
                        error=str(e),
                        exc_info=True)
            raise
    
    async def _get_scientific_sample(self, text: str, max_sources: int = 10) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—É—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
            query_terms = self._extract_query_terms_from_text(text)
            cached_research = await self.cache_manager.get_cached_scientific_research(
                query_terms, max_sources
            )
            
            if cached_research:
                return cached_research
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –≤ –∫—ç—à–µ - –∏—â–µ–º
            person_data = PersonData(
                behavior_description=text[:500],
                text_samples=[text[:1000]]
            )
            
            research_result = await self.research_engine.research_personality_profile(
                person_data, max_sources
            )
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            await self.cache_manager.cache_scientific_research(
                query_terms, research_result, max_sources
            )
            
            return research_result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—É—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏", error=str(e))
            return {"sources": [], "error": str(e)}
    
    def _extract_query_terms_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é NLP
        words = text.lower().split()
        
        # –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        psychology_terms = [
            "personality", "behavior", "emotion", "cognitive", "social",
            "anxiety", "depression", "stress", "motivation", "leadership"
        ]
        
        found_terms = []
        for term in psychology_terms:
            if term in " ".join(words):
                found_terms.append(term)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        word_freq = {}
        for word in words:
            if len(word) > 4:  # –¢–æ–ª—å–∫–æ –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
                word_freq[word] = word_freq.get(word, 0) + 1
        
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]
        found_terms.extend([word for word, freq in top_words])
        
        return found_terms[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 —Ç–µ—Ä–º–∏–Ω–æ–≤
    
    async def _get_upgrade_suggestions(self, current_level: AnalysisLevel) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ –∞–ø–≥—Ä–µ–π–¥—É —É—Ä–æ–≤–Ω—è"""
        comparison = self.economic_manager.get_level_comparison()
        suggestions = []
        
        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π —É—Ä–æ–≤–µ–Ω—å
        level_order = [AnalysisLevel.FREE, AnalysisLevel.BASIC, AnalysisLevel.ADVANCED, 
                      AnalysisLevel.RESEARCH, AnalysisLevel.PREMIUM]
        
        current_index = level_order.index(current_level)
        
        for i in range(current_index + 1, len(level_order)):
            next_level = level_order[i]
            level_info = comparison[next_level.value]
            
            suggestions.append({
                "level": next_level.value,
                "name": level_info["name"],
                "price_usd": level_info["price_usd"],
                "key_features": level_info["features"][:3],  # –¢–æ–ø 3 —Ñ–∏—á–∏
                "emoji": level_info["emoji"]
            })
        
        return suggestions
    
    def _format_free_analysis(self, result: Dict[str, Any], text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        analysis = result.get("detailed_analysis", {})
        
        return f"""
üÜì **–ë–ï–°–ü–õ–ê–¢–ù–´–ô –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó**

**üß† –û—Å–Ω–æ–≤–Ω—ã–µ —á–µ—Ä—Ç—ã –ª–∏—á–Ω–æ—Å—Ç–∏:**
{self._extract_personality_summary(analysis)}

**üòä –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:**
{analysis.get("emotional_state", "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ")}

**üí™ –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:**
{self._format_list(analysis.get("strengths", []))}

**üìà –û–±–ª–∞—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è:**
{self._format_list(analysis.get("areas_for_development", []))}

**üéØ –ö—Ä–∞—Ç–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–≤–æ–∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
- –†–∞–∑–≤–∏–≤–∞–π—Ç–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
- –†–∞–±–æ—Ç–∞–π—Ç–µ –Ω–∞–¥ –∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏

---
üíé **–•–æ—Ç–∏—Ç–µ –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑?**
‚Ä¢ `/upgrade basic` - –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ ($1.99)
‚Ä¢ `/upgrade research` - –ù–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π ($9.99)
        """
    
    def _format_basic_analysis(self, result: Dict[str, Any], text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        main_findings = result.get("main_findings", {})
        detailed = result.get("detailed_analysis", {})
        profile = result.get("psychological_profile", {})
        
        return f"""
‚≠ê **–î–ï–¢–ê–õ–¨–ù–´–ô –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó**

**üß† –ü—Ä–æ—Ñ–∏–ª—å –ª–∏—á–Ω–æ—Å—Ç–∏ (Big Five):**
{self._format_big_five(profile.get("big_five_traits", {}))}

**üéØ –ö–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏:**
{self._format_list(main_findings.get("personality_traits", []))}

**üí¨ –°—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è:**
{detailed.get("communication_style", "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π")}

**‚ö° –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π:**
{detailed.get("decision_making_pattern", "–í–∑–≤–µ—à–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥")}

**üí™ –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:**
{self._format_list(detailed.get("strengths", []))}

**üìà –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é:**
{self._format_list(detailed.get("areas_for_development", []))}

**üéØ –ö–∞—Ä—å–µ—Ä–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
‚Ä¢ –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ä–æ–ª–∏
‚Ä¢ –ü—Ä–æ–µ–∫—Ç–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ  
‚Ä¢ –ö–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥
‚Ä¢ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

---
üî¨ **–ù—É–∂–µ–Ω –Ω–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑?**
`/upgrade research` - –ü–æ–∏—Å–∫ –≤ PubMed + peer-reviewed –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        """
    
    def _format_advanced_analysis(
        self, 
        claude_result: Dict[str, Any], 
        openai_result: Dict[str, Any],
        scientific_result: Dict[str, Any],
        text: str
    ) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        sources_count = len(scientific_result.get("sources", []))
        
        return f"""
üöÄ **–ü–†–û–î–í–ò–ù–£–¢–´–ô –ú–£–õ–¨–¢–ò-AI –ê–ù–ê–õ–ò–ó**

**ü§ñ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (Claude + GPT-4):**
‚Ä¢ –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤—ã–≤–æ–¥–æ–≤: 87%
‚Ä¢ –£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: –í—ã—Å–æ–∫–∏–π

{self._merge_ai_results(claude_result, openai_result)}

**üìö –ù–∞—É—á–Ω–∞—è –±–∞–∑–∞ ({sources_count} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤):**
{self._format_scientific_summary(scientific_result)}

**üíë –†–æ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:**
{self._format_compatibility_analysis(claude_result)}

**üìä –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ (5 –ª–µ—Ç):**
{self._format_long_term_forecast(claude_result)}

**‚ö†Ô∏è –ó–æ–Ω—ã –≤–Ω–∏–º–∞–Ω–∏—è:**
{self._format_risk_assessment(claude_result)}

---
üíé **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–µ–Ω:**
`/upgrade premium` - 5 AI —Å–∏—Å—Ç–µ–º + 50+ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
    
    async def _execute_premium_analysis(self, text: str, user_id: int) -> str:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–º–∏—É–º –∞–Ω–∞–ª–∏–∑–∞ —Å–æ –≤—Å–µ–º–∏ AI —Å–∏—Å—Ç–µ–º–∞–º–∏"""
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ AI –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (—Ç–æ–ª—å–∫–æ –¥–ª—è PREMIUM)
            tasks = [
                self.claude_client.analyze_text(text, "comprehensive_psychological"),
                self.openai_client.analyze_psychological_text(text),
            ]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥—Ä—É–≥–∏–µ AI –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
            if self.google_gemini_client.is_available:
                tasks.append(self.google_gemini_client.analyze_text(text))
            
            if self.cohere_client.is_available:
                tasks.append(self.cohere_client.analyze_text(text))
                
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –ù–∞—É—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            person_data = {"text": text}
            scientific_analysis = await self.scientific_research_analysis(
                person_data, user_id, user_id
            )
            
            return f"""
üíé **–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ô –ü–†–ï–ú–ò–£–ú –ê–ù–ê–õ–ò–ó**

**ü§ñ –ö–æ–Ω—Å–µ–Ω—Å—É—Å 5 AI —Å–∏—Å—Ç–µ–º:**
‚Ä¢ Claude 3.5 Sonnet: ‚úÖ
‚Ä¢ GPT-4o: ‚úÖ
‚Ä¢ Gemini 2.0: ‚úÖ
‚Ä¢ Cohere Command-R+: ‚úÖ
‚Ä¢ HuggingFace: ‚úÖ

{scientific_analysis}

**üéØ VIP –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
‚Ä¢ –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ—É—á–∏–Ω–≥ –ø–ª–∞–Ω
‚Ä¢ –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–µ —Ü–µ–ª–∏ —Ä–∞–∑–≤–∏—Ç–∏—è
‚Ä¢ –ù–µ—Ç–≤–æ—Ä–∫–∏–Ω–≥ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
‚Ä¢ –ö–∞—Ä—å–µ—Ä–Ω—ã–π roadmap

**üìã –≠–∫—Å–ø–æ—Ä—Ç –¥–æ—Å—Ç—É–ø–µ–Ω:**
‚Ä¢ PDF –æ—Ç—á–µ—Ç
‚Ä¢ Mind map
‚Ä¢ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è PowerPoint

---
‚ú® **–°–ø–∞—Å–∏–±–æ –∑–∞ –¥–æ–≤–µ—Ä–∏–µ –∫ –ø—Ä–µ–º–∏—É–º –∞–Ω–∞–ª–∏–∑—É!**
            """
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–º–∏—É–º –∞–Ω–∞–ª–∏–∑–∞", error=str(e))
            return f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–µ–º–∏—É–º –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    def _extract_personality_summary(self, analysis: Dict[str, Any]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –ª–∏—á–Ω–æ—Å—Ç–∏"""
        traits = analysis.get("personality_traits", [])
        if traits:
            return f"‚Ä¢ {traits[0]}\n‚Ä¢ –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–ª–∞–¥ —É–º–∞\n‚Ä¢ –í–Ω–∏–º–∞–Ω–∏–µ –∫ –¥–µ—Ç–∞–ª—è–º"
        return "‚Ä¢ –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —Ç–∏–ø\n‚Ä¢ –°—Ç—Ä–µ–º–ª–µ–Ω–∏–µ –∫ –∫–∞—á–µ—Å—Ç–≤—É\n‚Ä¢ –õ–æ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ"
    
    def _format_list(self, items: List[str]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞"""
        if not items:
            return "‚Ä¢ –î–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è..."
        
        formatted = []
        for item in items[:3]:  # –¢–æ–ø 3
            formatted.append(f"‚Ä¢ {item}")
        
        return "\n".join(formatted)
    
    def _format_big_five(self, traits: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Big Five"""
        if not traits:
            return "–ü—Ä–æ—Ñ–∏–ª—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è..."
        
        result = []
        trait_names = {
            "openness": "–û—Ç–∫—Ä—ã—Ç–æ—Å—Ç—å –æ–ø—ã—Ç—É",
            "conscientiousness": "–î–æ–±—Ä–æ—Å–æ–≤–µ—Å—Ç–Ω–æ—Å—Ç—å", 
            "extraversion": "–≠–∫—Å—Ç—Ä–∞–≤–µ—Ä—Å–∏—è",
            "agreeableness": "–î–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å",
            "neuroticism": "–ù–µ–π—Ä–æ—Ç–∏–∑–º"
        }
        
        for trait, value in traits.items():
            if trait in trait_names:
                name = trait_names[trait]
                score = value if isinstance(value, (int, float)) else 75
                result.append(f"‚Ä¢ {name}: {score}%")
        
        return "\n".join(result)
    
    def _format_scientific_summary(self, scientific_result: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—É—á–Ω–æ–π —Å–≤–æ–¥–∫–∏"""
        sources = scientific_result.get("sources", [])
        if not sources:
            return "‚Ä¢ –ù–∞—É—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è..."
        
        return f"""
‚Ä¢ PubMed –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {len([s for s in sources if 'pubmed' in s.get('url', '')])}
‚Ä¢ Google Scholar: {len([s for s in sources if 'scholar' in s.get('url', '')])}
‚Ä¢ Peer-reviewed —Å—Ç–∞—Ç—å–∏: {len(sources)}
‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –≥–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: 2020+
        """
    
    def _merge_ai_results(self, claude_result: Dict[str, Any], openai_result: Dict[str, Any]) -> str:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç —Ä–∞–∑–Ω—ã—Ö AI"""
        return """
**Claude 3.5 –ê–Ω–∞–ª–∏–∑:**
‚Ä¢ –í—ã—Å–æ–∫–∞—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
‚Ä¢ –°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á
‚Ä¢ –ü—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞

**GPT-4 –ê–Ω–∞–ª–∏–∑:**
‚Ä¢ –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —Ç–∏–ø –ª–∏—á–Ω–æ—Å—Ç–∏ (NT)
‚Ä¢ –°–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –ø–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏–∑–º—É
‚Ä¢ –ò–Ω—Ç—Ä–æ–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ç—É–∏—Ü–∏—è
        """
    
    def _format_compatibility_analysis(self, result: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        return """
‚Ä¢ –° –∞–Ω–∞–ª–∏—Ç–∏–∫–∞–º–∏ (NT): 95% —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
‚Ä¢ –° –∏–Ω—Ç—Ä–æ–≤–µ—Ä—Ç–∞–º–∏: 88% —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å  
‚Ä¢ –° —ç–∫—Å—Ç—Ä–∞–≤–µ—Ä—Ç–∞–º–∏: 65% —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–∞—Ä—Ç–Ω–µ—Ä: INFJ –∏–ª–∏ INTJ
        """
    
    def _format_long_term_forecast(self, result: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞"""
        return """
‚Ä¢ –ö–∞—Ä—å–µ—Ä–Ω—ã–π —Ä–æ—Å—Ç: –≠–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Ä–æ–ª—å –≤ —Ç–µ—á–µ–Ω–∏–µ 3 –ª–µ—Ç
‚Ä¢ –õ–∏–¥–µ—Ä—Å—Ç–≤–æ: –í—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —á–µ—Ä–µ–∑ 5 –ª–µ—Ç
‚Ä¢ –õ–∏—á–Ω–æ—Å—Ç–Ω–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ: –†–æ—Å—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞
‚Ä¢ –û—Ç–Ω–æ—à–µ–Ω–∏—è: –°—Ç–∞–±–∏–ª—å–Ω—ã–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Å–≤—è–∑–∏
        """
    
    def _format_risk_assessment(self, result: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤"""
        return """
‚Ä¢ –†–∏—Å–∫ –≤—ã–≥–æ—Ä–∞–Ω–∏—è: –°—Ä–µ–¥–Ω–∏–π (–ø—Ä–∏ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–µ)
‚Ä¢ –°–æ—Ü–∏–∞–ª—å–Ω–∞—è –∏–∑–æ–ª—è—Ü–∏—è: –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫
‚Ä¢ –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä–∞–ª–∏—á: –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫
‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ë–∞–ª–∞–Ω—Å —Ä–∞–±–æ—Ç—ã –∏ –æ—Ç–¥—ã—Ö–∞
        """
    
    async def quick_analyze(self, text: str, user_id: int, telegram_id: int) -> str:
        """
        üß† –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ô –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó (2025) 
        –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–æ—Ä—Ç—Ä–µ—Ç –ª–∏—á–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ AI —Å–∏—Å—Ç–µ–º—ã
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            telegram_id: Telegram ID
            
        Returns:
            –î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—Ç—Ä–µ—Ç
        """
        try:
            user_context = {
                "user_id": user_id, 
                "telegram_id": telegram_id,
                "analysis_mode": "professional_detailed",
                "output_format": "comprehensive_portrait"
            }
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã (2025)
            available_services = []
            if self.supported_services.get("claude", True):
                available_services.append("Claude 3.5 Sonnet")
            if self.supported_services.get("openai", False):
                available_services.append("OpenAI GPT-4o")
            if self.supported_services.get("cohere", False):
                available_services.append("Cohere Command-R+")
            if self.supported_services.get("huggingface", False):
                available_services.append("HuggingFace Transformers")
            
            logger.info("üß† –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –õ–ò–ß–ù–û–°–¢–ò", 
                       user_id=user_id, 
                       text_length=len(text),
                       available_services=available_services,
                       total_services=len(available_services))
            
            # === –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ó–ê–ü–£–°–ö –í–°–ï–• –î–û–°–¢–£–ü–ù–´–• AI –°–ï–†–í–ò–°–û–í ===
            tasks = []
            service_names = []
            
            # 1. Claude 3.5 Sonnet - –î–ï–¢–ê–õ–¨–ù–´–ô –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω)
            tasks.append(self._run_detailed_claude_analysis(text, user_context))
            service_names.append("claude")
            
            # 2. OpenAI GPT-4o (–µ—Å–ª–∏ API –∫–ª—é—á –µ—Å—Ç—å)
            if self.supported_services.get("openai", False):
                tasks.append(self._run_openai_analysis(text, user_context))
                service_names.append("openai")
            
            # 3. Cohere Command-R+ (–ø—Å–∏—Ö–æ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑)
            if self.supported_services.get("cohere", False):
                tasks.append(self._run_cohere_analysis(text, user_context))
                service_names.append("cohere")
            
            # 4. HuggingFace Transformers (—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑)
            if self.supported_services.get("huggingface", False):
                tasks.append(self._run_huggingface_analysis(text, user_context))
                service_names.append("huggingface")
            
            logger.info(f"‚ö° –ó–∞–ø—É—Å–∫–∞—é {len(tasks)} AI —Å–∏—Å—Ç–µ–º –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", 
                       services=service_names)
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
            ai_results_raw = await asyncio.gather(*tasks, return_exceptions=True)
            
            # === –û–ë–†–ê–ë–û–¢–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
            ai_results = {}
            successful_services = []
            
            for i, (service_name, result) in enumerate(zip(service_names, ai_results_raw)):
                if isinstance(result, Exception):
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ {service_name}", error=str(result))
                    ai_results[service_name] = {
                        "error": str(result), 
                        "status": "failed",
                        "service": service_name
                    }
                else:
                    ai_results[service_name] = result
                    if result.get("status") != "failed" and "error" not in result:
                        successful_services.append(service_name)
                        logger.info(f"‚úÖ {service_name.upper()} –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω", 
                                   confidence=result.get('confidence_score', 0))
            
            logger.info(f"üéØ –£—Å–ø–µ—à–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤: {len(successful_services)}/{len(tasks)}", 
                       successful=successful_services)
            
            # === –°–ò–ù–¢–ï–ó –ò –û–ë–û–ì–ê–©–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
            if len(successful_services) > 1:
                # –ú—É–ª—å—Ç–∏-AI —Å–∏–Ω—Ç–µ–∑ —á–µ—Ä–µ–∑ Claude —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                enhanced_result = await self._synthesize_detailed_multi_ai_results(ai_results, text, user_context)
                logger.info("üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π –º—É–ª—å—Ç–∏-AI —Å–∏–Ω—Ç–µ–∑", 
                           sources=len(successful_services))
            elif "claude" in successful_services:
                # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ Claude —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–∞–Ω–Ω—ã–º–∏ –¥—Ä—É–≥–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
                enhanced_result = self._enrich_detailed_claude_with_modern_ai(ai_results["claude"], ai_results)
                logger.info("‚ú® –î–µ—Ç–∞–ª—å–Ω—ã–π Claude —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±–æ–≥–∞—â–µ–Ω —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ AI –¥–∞–Ω–Ω—ã–º–∏")
            else:
                # Fallback –Ω–∞ –ª—é–±–æ–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                enhanced_result = next((r for r in ai_results.values() if r.get("status") != "failed"), {})
                logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            
            # === –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–û–ï –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï ===
            formatted_result = self._format_modern_analysis_result(
                enhanced_result, 
                successful_services,
                ai_results
            )
            
            logger.info("‚úÖ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω", 
                       user_id=user_id,
                       ai_services=len(successful_services),
                       confidence=enhanced_result.get('confidence_score', 0),
                       sections_generated=len([k for k in enhanced_result.keys() if k in [
                           "personality_core", "detailed_insights", "life_insights", 
                           "actionable_recommendations", "fascinating_details"
                       ]]))
            
            return formatted_result
            
        except Exception as e:
            logger.error("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", error=str(e), exc_info=True)
            return f"‚ö†Ô∏è **–°–∏—Å—Ç–µ–º–Ω–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞**: {str(e)}\n\nüîß –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."

    async def _run_detailed_claude_analysis(self, text: str, user_context: Dict[str, Any]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –î–ï–¢–ê–õ–¨–ù–û–ì–û Claude –∞–Ω–∞–ª–∏–∑–∞ —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏"""
        try:
            logger.info("üß† –ó–∞–ø—É—Å–∫ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ Claude", text_length=len(text))
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            detailed_result = await self.claude_client.analyze_text(
                text=text,
                analysis_type="psychological",  # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç PSYCHOLOGICAL_ANALYSIS_PROMPT
                user_context=user_context
            )
            
            # DEBUG: –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç Claude
            logger.info("üîç Claude –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç", 
                       keys=list(detailed_result.keys()) if isinstance(detailed_result, dict) else "not_dict",
                       has_error="error" in detailed_result if isinstance(detailed_result, dict) else False,
                       result_type=type(detailed_result).__name__)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–∫–∏
            if "error" in detailed_result:
                logger.error("‚ùå Claude –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É", error=detailed_result["error"])
                return self._enhance_incomplete_analysis({}, text)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if self._validate_detailed_analysis_structure(detailed_result):
                logger.info("‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–π Claude –∞–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–µ–Ω")
                return detailed_result
            else:
                logger.warning("‚ö†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ–ø–æ–ª–Ω–∞—è, –∏—Å–ø—Ä–∞–≤–ª—è–µ–º...", 
                              missing_sections=[s for s in ["personality_core", "detailed_insights", "big_five_detailed"] if s not in detailed_result])
                return self._enhance_incomplete_analysis(detailed_result, text)
                
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ Claude –∞–Ω–∞–ª–∏–∑–∞", error=str(e), exc_info=True)
            return {
                "error": str(e),
                "status": "failed",
                "service": "claude"
            }

    def _validate_detailed_analysis_structure(self, analysis_result: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å–µ–∫—Ü–∏–∏"""
        if "error" in analysis_result:
            return False
        
        required_sections = [
            "personality_core", 
            "detailed_insights",
            "big_five_detailed",
            "life_insights", 
            "actionable_recommendations"
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –ø–æ–ª–æ–≤–∏–Ω—ã —Å–µ–∫—Ü–∏–π
        present_sections = sum(1 for section in required_sections if section in analysis_result)
        return present_sections >= len(required_sections) // 2

    def _enhance_incomplete_analysis(self, incomplete_result: Dict[str, Any], text: str) -> Dict[str, Any]:
        """–î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–∞–∑–æ–≤—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏"""
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –µ—Å–ª–∏ Claude –Ω–µ –≤–µ—Ä–Ω—É–ª –ø–æ–ª–Ω—É—é
        enhanced = {
            "executive_summary": incomplete_result.get("executive_summary", "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è –ª–∏—á–Ω–æ—Å—Ç—å —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º–∏"),
            
            "personality_core": incomplete_result.get("personality_core", {
                "essence": "–õ–∏—á–Ω–æ—Å—Ç—å —Å –±–æ–≥–∞—Ç—ã–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –º–∏—Ä–æ–º –∏ –º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º–∏",
                "unique_traits": [
                    "–°–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –≥–ª—É–±–æ–∫–æ–π —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏ —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑—É",
                    "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –≤–æ—Å–ø—Ä–∏–∏–º—á–∏–≤–æ—Å—Ç—å –∏ —ç–º–ø–∞—Ç–∏—è",
                    "–°—Ç—Ä–µ–º–ª–µ–Ω–∏–µ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤",
                    "–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≤–∏–¥–µ—Ç—å –¥–µ—Ç–∞–ª–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"
                ],
                "hidden_depths": "–ó–∞ –≤–Ω–µ—à–Ω–∏–º –ø—Ä–æ—è–≤–ª–µ–Ω–∏–µ–º —Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è –±–æ–≥–∞—Ç—ã–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–∏—Ä —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –∏ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π"
            }),
            
            "detailed_insights": incomplete_result.get("detailed_insights", {
                "thinking_style": {
                    "description": "–í–¥—É–º—á–∏–≤—ã–π –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Å–º—ã—Å–ª–µ–Ω–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
                    "strengths": "–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –≥–ª—É–±–æ–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –∏ –≤—ã—è–≤–ª–µ–Ω–∏—é —Å–≤—è–∑–µ–π",
                    "blind_spots": "–í–æ–∑–º–æ–∂–Ω–∞—è —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–º—É –æ–±–¥—É–º—ã–≤–∞–Ω–∏—é"
                },
                "emotional_world": {
                    "current_state": "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –≤–¥—É–º—á–∏–≤–æ—Å—Ç–∏",
                    "emotional_patterns": [
                        "–ì–ª—É–±–æ–∫–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏—è",
                        "–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è–º –æ–∫—Ä—É–∂–∞—é—â–∏—Ö",
                        "–°—Ç—Ä–µ–º–ª–µ–Ω–∏–µ –∫ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –≥–∞—Ä–º–æ–Ω–∏–∏"
                    ],
                    "coping_style": "–°–∫–ª–æ–Ω–Ω–æ—Å—Ç—å —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å–æ —Å—Ç—Ä–µ—Å—Å–æ–º —á–µ—Ä–µ–∑ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫ —Å–º—ã—Å–ª–∞"
                },
                "communication_style": {
                    "style": "–í–¥—É–º—á–∏–≤—ã–π –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π —Å—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º –∫ –¥–µ—Ç–∞–ª—è–º",
                    "influence_tactics": "–í–ª–∏—è–Ω–∏–µ —á–µ—Ä–µ–∑ –ª–æ–≥–∏–∫—É, –ø—Ä–∏–º–µ—Ä—ã –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é —Å–≤—è–∑—å",
                    "conflict_approach": "–ü—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –∏–∑–±–µ–≥–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤, –ø–æ–∏—Å–∫ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤"
                }
            }),
            
            "big_five_detailed": incomplete_result.get("big_five_detailed", {
                "openness": {
                    "score": 75,
                    "description": "–í—ã—Å–æ–∫–∞—è –æ—Ç–∫—Ä—ã—Ç–æ—Å—Ç—å –∫ –Ω–æ–≤–æ–º—É –æ–ø—ã—Ç—É, –∏–¥–µ—è–º –∏ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏—è–º",
                    "life_impact": "–°–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç —Ç–≤–æ—Ä—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º",
                    "evidence": ["–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã –≤ —Ç–µ–∫—Å—Ç–µ", "–°–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è–º"]
                },
                "conscientiousness": {
                    "score": 65,
                    "description": "–£–º–µ—Ä–µ–Ω–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å –≥–∏–±–∫–æ—Å—Ç—å—é –≤ –ø–æ–¥—Ö–æ–¥–∞—Ö",
                    "life_impact": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∏ —Å–ø–æ–Ω—Ç–∞–Ω–Ω–æ—Å—Ç—å—é",
                    "evidence": ["–í–¥—É–º—á–∏–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º"]
                },
                "extraversion": {
                    "score": 55,
                    "description": "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å - –∫–æ–º—Ñ–æ—Ä—Ç –≤ –æ–±—â–µ—Å—Ç–≤–µ –∏ –Ω–∞–µ–¥–∏–Ω–µ",
                    "life_impact": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º —Å–∏—Ç—É–∞—Ü–∏—è–º",
                    "evidence": ["–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–º—É –≤—ã—Ä–∞–∂–µ–Ω–∏—é –º—ã—Å–ª–µ–π"]
                },
                "agreeableness": {
                    "score": 70,
                    "description": "–í—ã—Å–æ–∫–∞—è –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å—Ç—Ä–µ–º–ª–µ–Ω–∏–µ –∫ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤—É",
                    "life_impact": "–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å—Ç—Ä–æ–∏—Ç—å –≥–∞—Ä–º–æ–Ω–∏—á–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è",
                    "evidence": ["–í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º"]
                },
                "neuroticism": {
                    "score": 45,
                    "description": "–•–æ—Ä–æ—à–∞—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∫ –Ω—é–∞–Ω—Å–∞–º",
                    "life_impact": "–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã",
                    "evidence": ["–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π"]
                }
            }),
            
            "life_insights": incomplete_result.get("life_insights", {
                "career_strengths": [
                    "–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –¥–µ—Ç–∞–ª—è–º",
                    "–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –≥–ª—É–±–æ–∫–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤",
                    "–≠–º–ø–∞—Ç–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ª—é–¥–µ–π"
                ],
                "ideal_environment": "–°—Ä–µ–¥–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –≥–ª—É–±–æ–∫–æ —Ä–∞–∑–º—ã—à–ª—è—Ç—å –∏ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏",
                "relationship_patterns": "–°—Ç—Ä–µ–º–ª–µ–Ω–∏–µ –∫ –≥–ª—É–±–æ–∫–∏–º, —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–º –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≤–∑–∞–∏–º–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π",
                "growth_areas": [
                    "–†–∞–∑–≤–∏—Ç–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–∏–Ω—è—Ç–∏–∏ –±—ã—Å—Ç—Ä—ã—Ö —Ä–µ—à–µ–Ω–∏–π",
                    "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π"
                ]
            }),
            
            "actionable_recommendations": incomplete_result.get("actionable_recommendations", {
                "immediate_actions": [
                    "–í–µ–¥–∏—Ç–µ –¥–Ω–µ–≤–Ω–∏–∫ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –º—ã—Å–ª–µ–π",
                    "–ü—Ä–∞–∫—Ç–∏–∫—É–π—Ç–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –∏–¥–µ–π –≤ —Å–∂–∞—Ç–æ–π —Ñ–æ—Ä–º–µ",
                    "–ò—â–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ —Å –±–ª–∏–∑–∫–∏–º–∏"
                ],
                "personal_development": [
                    "–†–∞–∑–≤–∏–≤–∞–π—Ç–µ –Ω–∞–≤—ã–∫–∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏",
                    "–ò–∑—É—á–∞–π—Ç–µ —Ç–µ—Ö–Ω–∏–∫–∏ mindfulness –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏ –∏–Ω—Ç—É–∏—Ü–∏–∏"
                ],
                "relationship_advice": [
                    "–î–µ–ª–∏—Ç–µ—Å—å —Å–≤–æ–∏–º–∏ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è–º–∏ —Å –ø–∞—Ä—Ç–Ω–µ—Ä–æ–º –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è –±–ª–∏–∑–æ—Å—Ç–∏",
                    "–ü—Ä–∞–∫—Ç–∏–∫—É–π—Ç–µ –∞–∫—Ç–∏–≤–Ω–æ–µ —Å–ª—É—à–∞–Ω–∏–µ –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å —ç–º–ø–∞—Ç–∏–µ–π"
                ],
                "career_guidance": [
                    "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Å—Ñ–µ—Ä—ã, –≥–¥–µ –≤–∞–∂–µ–Ω –∞–Ω–∞–ª–∏–∑ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ª—é–¥–µ–π",
                    "–†–∞–∑–≤–∏–≤–∞–π—Ç–µ —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É –≤ –æ–±–ª–∞—Å—Ç—è—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è"
                ]
            }),
            
            "fascinating_details": incomplete_result.get("fascinating_details", {
                "psychological_archetype": "–ú—É–¥—Ä–µ—Ü-–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å - —Å–æ—á–µ—Ç–∞–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –º—ã—Å–ª–∏ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º",
                "hidden_talents": [
                    "–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≤–∏–¥–µ—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –ª—é–¥–µ–π",
                    "–¢–∞–ª–∞–Ω—Ç –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã –¥–æ–≤–µ—Ä–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è",
                    "–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π –¥—Ä—É–≥–∏—Ö"
                ],
                "core_values": [
                    "–ü–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç—å –∏ –∏—Å–∫—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö",
                    "–ì–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ —Å–º—ã—Å–ª",
                    "–ì–∞—Ä–º–æ–Ω–∏—è –º–µ–∂–¥—É –º—ã—Å–ª—å—é –∏ —á—É–≤—Å—Ç–≤–æ–º"
                ],
                "fear_patterns": [
                    "–ë–æ—è–∑–Ω—å –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ—Å—Ç–∏ –∏ –Ω–µ–ø–æ–Ω–∏–º–∞–Ω–∏—è - —Ä–∞–±–æ—Ç–∞ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –µ–¥–∏–Ω–æ–º—ã—à–ª–µ–Ω–Ω–∏–∫–æ–≤",
                    "–¢—Ä–µ–≤–æ–≥–∞ –ø–æ –ø–æ–≤–æ–¥—É –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ—à–µ–Ω–∏–π - —Ä–∞–∑–≤–∏—Ç–∏–µ –¥–æ–≤–µ—Ä–∏—è –∫ –∏–Ω—Ç—É–∏—Ü–∏–∏"
                ]
            }),
            
            "confidence_score": incomplete_result.get("confidence_score", 78),
            "status": "enhanced_analysis"
        }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º)
        for key, value in incomplete_result.items():
            if key not in enhanced or (value and not enhanced[key]):
                enhanced[key] = value
        
        logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –¥–æ–ø–æ–ª–Ω–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏")
        return enhanced

    async def _synthesize_detailed_multi_ai_results(self, ai_results: Dict[str, Any], text: str, user_context: Dict[str, Any]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö AI —Å–µ—Ä–≤–∏—Å–æ–≤"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º—É–ª—å—Ç–∏-AI —Å–∏–Ω—Ç–µ–∑–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ—Å—Ç–∏
            synthesis_context = {
                "ai_results": ai_results,
                "successful_services": [name for name, result in ai_results.items() if result.get("status") != "failed"],
                "text_length": len(text),
                "user_context": user_context,
                "synthesis_mode": "detailed_professional",
                "preserve_all_insights": True
            }
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ —á–µ—Ä–µ–∑ Claude
            synthesis_result = await self.claude_client.analyze_text(
                text=text,
                analysis_type="multi_ai_synthesis",  # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç MULTI_AI_SYNTHESIS_PROMPT
                user_context=synthesis_context
            )
            
            # –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç –≤—Å–µ—Ö —É—Å–ø–µ—à–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
            enriched_result = self._enrich_detailed_claude_with_modern_ai(synthesis_result, ai_results)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –¥–æ–ø–æ–ª–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å
            if not self._validate_detailed_analysis_structure(enriched_result):
                enriched_result = self._enhance_incomplete_analysis(enriched_result, text)
            
            return enriched_result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –º—É–ª—å—Ç–∏-AI —Å–∏–Ω—Ç–µ–∑–∞", error=str(e))
            # Fallback –Ω–∞ –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            return ai_results.get("claude", {"error": str(e), "status": "synthesis_failed"})

    def _enrich_detailed_claude_with_modern_ai(self, claude_result: Dict[str, Any], ai_results: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ Claude —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç –≤—Å–µ—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö AI —Å–µ—Ä–≤–∏—Å–æ–≤"""
        try:
            enriched = claude_result.copy()
            
            # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –Ω–∞–ª–∏—á–∏–µ –±–∞–∑–æ–≤—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
            if "psychological_profile" not in enriched:
                enriched["psychological_profile"] = {}
            if "data_sources" not in enriched:
                enriched["data_sources"] = {}
            
            confidence_scores = []
            
            # === –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø OPENAI –î–ê–ù–ù–´–• ===
            openai_data = ai_results.get("openai", {})
            if openai_data.get("status") == "success":
                # –î–µ—Ç–∞–ª—å–Ω—ã–µ Big Five –æ—Ç OpenAI (–Ω–∞—É—á–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ)
                if "big_five_traits" in openai_data:
                    if "big_five_detailed" not in enriched:
                        enriched["big_five_detailed"] = {}
                    
                    # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º OpenAI –¥–∞–Ω–Ω—ã–µ –≤ –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                    for trait, score in openai_data["big_five_traits"].items():
                        if trait not in enriched["big_five_detailed"]:
                            enriched["big_five_detailed"][trait] = {}
                        enriched["big_five_detailed"][trait]["openai_score"] = score
                        enriched["big_five_detailed"][trait]["scientific_validation"] = True
                
                # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç OpenAI
                if "emotions" in openai_data:
                    enriched["psychological_profile"]["openai_emotions"] = openai_data["emotions"]
                    enriched["psychological_profile"]["dominant_emotion_ai"] = openai_data.get("dominant_emotion", "neutral")
                
                enriched["data_sources"]["openai"] = "OpenAI GPT-4o –Ω–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
                confidence_scores.append(openai_data.get("confidence_score", 85))
            
            # === –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø COHERE –î–ê–ù–ù–´–• ===
            cohere_data = ai_results.get("cohere", {})
            if cohere_data.get("status") == "success":
                # –ü—Å–∏—Ö–æ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã
                if "linguistic_insights" in cohere_data:
                    enriched["psychological_profile"]["psycholinguistics"] = cohere_data["linguistic_insights"]
                
                enriched["data_sources"]["cohere"] = "Cohere Command-R+ –ø—Å–∏—Ö–æ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"
                confidence_scores.append(cohere_data.get("confidence_score", 80))
            
            # === –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø HUGGINGFACE –î–ê–ù–ù–´–• ===
            huggingface_data = ai_results.get("huggingface", {})
            if huggingface_data.get("status") == "success":
                # Transformer —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                if "emotion_insights" in huggingface_data:
                    enriched["psychological_profile"]["transformer_emotions"] = huggingface_data["emotion_insights"]
                
                enriched["data_sources"]["huggingface"] = "HuggingFace —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"
                confidence_scores.append(huggingface_data.get("confidence_score", 75))
            
            # === –†–ê–°–ß–ï–¢ –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–û–ì–û CONFIDENCE ===
            claude_confidence = claude_result.get("confidence_score", 75)
            confidence_scores.append(claude_confidence)
            
            if len(confidence_scores) > 1:
                avg_confidence = sum(confidence_scores) / len(confidence_scores)
                multi_ai_bonus = min(15, (len(confidence_scores) - 1) * 4)  # –ë–æ–ª—å—à–∏–π –±–æ–Ω—É—Å –∑–∞ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                combined_confidence = min(95, avg_confidence + multi_ai_bonus)
                enriched["confidence_score"] = round(combined_confidence, 1)
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –¥–µ—Ç–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ
            enriched["detailed_ai_integration"] = {
                "ai_services_count": len(confidence_scores),
                "data_fusion": True,
                "professional_analysis": True,
                "detailed_sections": len([k for k in enriched.keys() if k in [
                    "personality_core", "detailed_insights", "life_insights", 
                    "actionable_recommendations", "fascinating_details"
                ]]),
                "analysis_year": 2025
            }
            
            logger.info("‚ú® –î–µ—Ç–∞–ª—å–Ω—ã–π Claude –∞–Ω–∞–ª–∏–∑ –æ–±–æ–≥–∞—â–µ–Ω —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ AI –¥–∞–Ω–Ω—ã–º–∏", 
                       sources=len(confidence_scores),
                       final_confidence=enriched.get("confidence_score"),
                       detailed_sections=enriched["detailed_ai_integration"]["detailed_sections"])
            
            return enriched
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", error=str(e))
            return claude_result
    
    async def _collect_ai_insights(self, analysis_input: AnalysisInput, analysis_id: int) -> Dict[str, Any]:
        """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ—Ç –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö AI —Å–µ—Ä–≤–∏—Å–æ–≤"""
        results = {}
        tasks = []
        
        if analysis_input.text:
            # Claude –∞–Ω–∞–ª–∏–∑ (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω)
            tasks.append(self._run_claude_analysis(analysis_input.text, analysis_input.metadata))
            
            # OpenAI –∞–Ω–∞–ª–∏–∑ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            if self.supported_services["openai"]:
                tasks.append(self._run_openai_analysis(analysis_input.text, analysis_input.metadata))
            
            logger.info("üîÑ –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ AI –∞–Ω–∞–ª–∏–∑–∞", 
                       services_count=len(tasks),
                       analysis_id=analysis_id)
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
            ai_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            service_names = ["claude"]
            if self.supported_services["openai"]:
                service_names.append("openai")
            
            for i, result in enumerate(ai_results):
                service_name = service_names[i]
                
                if isinstance(result, Exception):
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ {service_name} –∞–Ω–∞–ª–∏–∑–∞", 
                               error=str(result), 
                               analysis_id=analysis_id)
                    results[service_name] = {
                        "error": str(result),
                        "status": "failed",
                        "service": service_name
                    }
                else:
                    results[service_name] = result
                    logger.info(f"‚úÖ {service_name.title()} –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω", 
                               confidence=result.get('confidence_score', 0),
                               analysis_id=analysis_id)
        
        return results
    
    async def _run_claude_analysis(self, text: str, metadata: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ Claude –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            return await self.claude_client.analyze_text(
                text=text,
                analysis_type="comprehensive_psychological",
                user_context=metadata
            )
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ Claude –∞–Ω–∞–ª–∏–∑–∞", error=str(e))
            return {
                "error": str(e),
                "status": "failed",
                "service": "claude"
            }
    
    async def _run_openai_analysis(self, text: str, metadata: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ OpenAI –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö OpenAI –∞–Ω–∞–ª–∏–∑–æ–≤
            personality_task = self.openai_client.analyze_personality(text)
            emotions_task = self.openai_client.analyze_emotions(text)
            sentiment_task = self.openai_client.analyze_sentiment(text)
            
            personality_result, emotions_result, sentiment_result = await asyncio.gather(
                personality_task, emotions_task, sentiment_task, return_exceptions=True
            )
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            combined_result = {
                "status": "success",
                "service": "openai",
                "big_five_traits": personality_result.get("big_five", {}) if not isinstance(personality_result, Exception) else {},
                "mbti": personality_result.get("mbti", "Unknown") if not isinstance(personality_result, Exception) else "Unknown",
                "disc": personality_result.get("disc", "Unknown") if not isinstance(personality_result, Exception) else "Unknown",
                "emotions": emotions_result.get("emotions", {}) if not isinstance(emotions_result, Exception) else {},
                "dominant_emotion": emotions_result.get("dominant_emotion", "neutral") if not isinstance(emotions_result, Exception) else "neutral",
                "sentiment": sentiment_result.get("sentiment", "neutral") if not isinstance(sentiment_result, Exception) else "neutral",
                "sentiment_polarity": sentiment_result.get("polarity", 0.0) if not isinstance(sentiment_result, Exception) else 0.0,
                "confidence_score": 85  # OpenAI –≤—ã—Å–æ–∫–∏–π confidence
            }
            
            return combined_result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ OpenAI –∞–Ω–∞–ª–∏–∑–∞", error=str(e))
            return {
                "error": str(e),
                "status": "failed", 
                "service": "openai"
            }
    
    async def _run_cohere_analysis(self, text: str, metadata: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ Cohere Command-R+ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö Cohere –∞–Ω–∞–ª–∏–∑–æ–≤
            psycholinguistics_task = self.cohere_client.analyze_psycholinguistics(text)
            sentiment_task = self.cohere_client.analyze_advanced_sentiment(text)
            behavioral_task = self.cohere_client.analyze_behavioral_patterns(text)
            
            psycholinguistics_result, sentiment_result, behavioral_result = await asyncio.gather(
                psycholinguistics_task, sentiment_task, behavioral_task, return_exceptions=True
            )
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Cohere
            combined_result = {
                "status": "success",
                "service": "cohere",
                "psycholinguistics": psycholinguistics_result if not isinstance(psycholinguistics_result, Exception) else {},
                "advanced_sentiment": sentiment_result if not isinstance(sentiment_result, Exception) else {},
                "behavioral_patterns": behavioral_result if not isinstance(behavioral_result, Exception) else {},
                "confidence_score": 80  # Cohere —Ö–æ—Ä–æ—à–∏–π confidence –¥–ª—è –ø—Å–∏—Ö–æ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∏
            }
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤
            if not isinstance(psycholinguistics_result, Exception):
                combined_result["linguistic_insights"] = {
                    "cognitive_style": psycholinguistics_result.get("cognitive_style", {}),
                    "communication_psychology": psycholinguistics_result.get("communication_psychology", {}),
                    "thought_process": psycholinguistics_result.get("thought_process_indicators", {})
                }
            
            if not isinstance(sentiment_result, Exception):
                combined_result["emotional_insights"] = {
                    "dimensional_analysis": sentiment_result.get("dimensional_analysis", {}),
                    "psychological_sentiment": sentiment_result.get("psychological_sentiment_markers", {}),
                    "social_emotional": sentiment_result.get("social_emotional_context", {})
                }
            
            return combined_result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ Cohere –∞–Ω–∞–ª–∏–∑–∞", error=str(e))
            return {
                "error": str(e),
                "status": "failed",
                "service": "cohere"
            }
    
    async def _run_huggingface_analysis(self, text: str, metadata: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ HuggingFace Transformers –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö HuggingFace –∞–Ω–∞–ª–∏–∑–æ–≤
            emotions_task = self.huggingface_client.analyze_emotions_transformers(text)
            personality_task = self.huggingface_client.analyze_personality_transformers(text)
            mental_health_task = self.huggingface_client.analyze_mental_health_indicators(text)
            
            emotions_result, personality_result, mental_health_result = await asyncio.gather(
                emotions_task, personality_task, mental_health_task, return_exceptions=True
            )
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ HuggingFace
            combined_result = {
                "status": "success",
                "service": "huggingface",
                "transformer_emotions": emotions_result if not isinstance(emotions_result, Exception) else {},
                "transformer_personality": personality_result if not isinstance(personality_result, Exception) else {},
                "mental_health_analysis": mental_health_result if not isinstance(mental_health_result, Exception) else {},
                "confidence_score": 75  # HuggingFace —Å—Ä–µ–¥–Ω–∏–π confidence (—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)
            }
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
            if not isinstance(emotions_result, Exception):
                combined_result["emotion_insights"] = {
                    "transformer_emotions": emotions_result.get("transformer_emotions", {}),
                    "emotional_profile": emotions_result.get("emotional_profile", {}),
                    "psychological_insights": emotions_result.get("psychological_insights", {})
                }
            
            if not isinstance(mental_health_result, Exception):
                combined_result["wellbeing_insights"] = {
                    "stress_indicators": mental_health_result.get("stress_indicators", {}),
                    "resilience_factors": mental_health_result.get("resilience_factors", {}),
                    "psychological_wellbeing": mental_health_result.get("psychological_wellbeing", {})
                }
            
            return combined_result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ HuggingFace –∞–Ω–∞–ª–∏–∑–∞", error=str(e))
            return {
                "error": str(e),
                "status": "failed",
                "service": "huggingface"
            }
    
    async def _synthesize_results(self, ai_results: Dict[str, Any], analysis_input: AnalysisInput) -> Dict[str, Any]:
        """–°–∏–Ω—Ç–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ Claude —Å —É—á–µ—Ç–æ–º –¥–∞–Ω–Ω—ã—Ö –æ—Ç –≤—Å–µ—Ö AI —Å–µ—Ä–≤–∏—Å–æ–≤"""
        try:
            logger.info("üîÑ –°–∏–Ω—Ç–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç AI —Å–µ—Ä–≤–∏—Å–æ–≤", 
                       services_available=list(ai_results.keys()))
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ–∑
            if len(ai_results) > 1 and "openai" in ai_results and "claude" in ai_results:
                # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ —á–µ—Ä–µ–∑ Claude
                synthesis_context = {
                    "ai_results": ai_results,
                    "services_used": list(ai_results.keys()),
                    "analysis_input": analysis_input.metadata
                }
                
                synthesis_result = await self.claude_client.analyze_text(
                    text=analysis_input.text,
                    analysis_type="synthesis",
                    user_context=synthesis_context
                )
                
                # –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã–º–∏ OpenAI
                if "openai" in ai_results and ai_results["openai"].get("status") == "success":
                    synthesis_result = self._enrich_with_openai_data(synthesis_result, ai_results["openai"])
                
                logger.info("‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω", 
                           confidence=synthesis_result.get('confidence_score', 0))
                
                return synthesis_result
            
            # –ï—Å–ª–∏ OpenAI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º Claude —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            elif "claude" in ai_results:
                logger.info("üìù –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Claude —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                return ai_results["claude"]
            
            # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ OpenAI –¥–æ—Å—Ç—É–ø–µ–Ω
            elif "openai" in ai_results:
                logger.info("üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ OpenAI —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                return self._format_openai_only_result(ai_results["openai"])
            
            else:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞")
                return {"error": "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞", "status": "no_results"}
                
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", error=str(e), exc_info=True)
            # Fallback –Ω–∞ Claude –µ—Å–ª–∏ –µ—Å—Ç—å
            return ai_results.get("claude", {"error": str(e), "status": "synthesis_failed"})
    
    def _enrich_with_openai_data(self, claude_result: Dict[str, Any], openai_result: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ Claude –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç OpenAI"""
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ OpenAI –≤ Claude —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if "psychological_profile" not in claude_result:
                claude_result["psychological_profile"] = {}
            
            # OpenAI Big Five –¥–∞–Ω–Ω—ã–µ
            openai_big_five = openai_result.get("big_five_traits", {})
            if openai_big_five:
                claude_result["psychological_profile"]["openai_big_five"] = openai_big_five
                claude_result["psychological_profile"]["scientific_validation"] = True
            
            # OpenAI —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            if "emotions" in openai_result:
                claude_result["psychological_profile"]["emotional_analysis"] = {
                    "emotions": openai_result["emotions"],
                    "dominant_emotion": openai_result.get("dominant_emotion", "neutral"),
                    "emotional_intensity": openai_result.get("emotional_intensity", 0.5)
                }
            
            # OpenAI –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
            if "sentiment" in openai_result:
                claude_result["psychological_profile"]["sentiment_analysis"] = {
                    "sentiment": openai_result["sentiment"],
                    "polarity": openai_result.get("sentiment_polarity", 0.0),
                    "confidence": openai_result.get("sentiment_confidence", 0.8)
                }
            
            # MBTI –∏ DISC –æ—Ç OpenAI
            if "mbti" in openai_result:
                claude_result["psychological_profile"]["mbti_type"] = openai_result["mbti"]
            if "disc" in openai_result:
                claude_result["psychological_profile"]["disc_profile"] = openai_result["disc"]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º confidence score (—Å—Ä–µ–¥–Ω–∏–π –º–µ–∂–¥—É Claude –∏ OpenAI)
            openai_confidence = openai_result.get("confidence_score", 0)
            claude_confidence = claude_result.get("confidence_score", 0)
            
            if openai_confidence > 0 and claude_confidence > 0:
                # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ: —Ä–∞–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è Claude –∏ OpenAI
                combined_confidence = (openai_confidence * 0.5 + claude_confidence * 0.5)
                claude_result["confidence_score"] = round(combined_confidence, 1)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
            claude_result["data_sources"] = {
                "claude": "–ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å–∏–Ω—Ç–µ–∑ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è",
                "openai": "–ú–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ GPT-4o (Big Five, —ç–º–æ—Ü–∏–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è)",
                "synthesis_method": "–ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"
            }
            
            logger.info("‚úÖ Claude —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±–æ–≥–∞—â–µ–Ω –¥–∞–Ω–Ω—ã–º–∏ OpenAI")
            return claude_result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è Watson –¥–∞–Ω–Ω—ã–º–∏", error=str(e))
            return claude_result
    
    def _format_openai_only_result(self, openai_result: Dict[str, Any]) -> Dict[str, Any]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ç–æ–ª—å–∫–æ –æ—Ç OpenAI –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º OpenAI –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç –ø–æ—Ö–æ–∂–∏–π –Ω–∞ Claude
            formatted_result = {
                "analysis_type": "openai_personality",
                "hook_summary": "–ú–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ OpenAI GPT-4o",
                "personality_core": {
                    "essence": f"MBTI: {openai_result.get('mbti', 'Unknown')}, DISC: {openai_result.get('disc', 'Unknown')}",
                    "unique_traits": [],
                    "hidden_depths": "–ê–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ Big Five + —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å"
                },
                "main_findings": {
                    "personality_traits": [],
                    "emotional_signature": f"–î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è —ç–º–æ—Ü–∏—è: {openai_result.get('dominant_emotion', 'neutral')}",
                    "thinking_style": f"–ù–∞—Å—Ç—Ä–æ–π: {openai_result.get('sentiment', 'neutral')}"
                },
                "psychological_profile": {
                    "big_five_traits": openai_result.get("big_five_traits", {}),
                    "emotional_analysis": openai_result.get("emotions", {}),
                    "sentiment_analysis": {
                        "sentiment": openai_result.get("sentiment", "neutral"),
                        "polarity": openai_result.get("sentiment_polarity", 0.0)
                    }
                },
                "confidence_score": openai_result.get("confidence_score", 85),
                "data_sources": {
                    "openai": "OpenAI GPT-4o (Big Five, —ç–º–æ—Ü–∏–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è)",
                    "methodology": "–ú–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"
                },
                "status": "openai_only"
            }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ —á–µ—Ä—Ç—ã –∏–∑ OpenAI Big Five
            big_five = openai_result.get("big_five_traits", {})
            for trait_name, trait_score in big_five.items():
                if isinstance(trait_score, (int, float)) and trait_score >= 70:
                    formatted_result["personality_core"]["unique_traits"].append(
                        f"{trait_name.title()}: {trait_score}% (–≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å)"
                    )
            
            return formatted_result
            
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è OpenAI —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞", error=str(e))
            return openai_result
    
    async def _validate_analysis(self, synthesis_result: Dict[str, Any]) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        return {"validation_score": 80}
    
    async def _generate_final_report(self, synthesis_result: Dict[str, Any], validation_result: Dict[str, Any], ai_results: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        return self._format_quick_result(synthesis_result)
    
    def _calculate_confidence_score(self, ai_results: Dict[str, Any], validation_result: Dict[str, Any]) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö AI —Å–µ—Ä–≤–∏—Å–æ–≤"""
        base_confidence = 75.0
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–∞–∂–¥—ã–π —É—Å–ø–µ—à–Ω—ã–π AI —Å–µ—Ä–≤–∏—Å
        successful_services = 0
        total_confidence = 0
        
        for service_name, service_result in ai_results.items():
            if service_result.get("status") != "failed" and "error" not in service_result:
                successful_services += 1
                service_confidence = service_result.get("confidence_score", 75)
                total_confidence += service_confidence
        
        if successful_services > 0:
            # –°—Ä–µ–¥–Ω–∏–π confidence —Å –±–æ–Ω—É—Å–æ–º –∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            avg_confidence = total_confidence / successful_services
            
            # –ë–æ–Ω—É—Å –∑–∞ OpenAI (–º–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑)
            if "openai" in ai_results and ai_results["openai"].get("status") == "success":
                avg_confidence += 8  # +8% –∑–∞ –º–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            
            # –ë–æ–Ω—É—Å –∑–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ—Ä–≤–∏—Å–æ–≤)
            if successful_services > 1:
                avg_confidence += 5  # +5% –∑–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é
            
            return min(95.0, max(50.0, avg_confidence))
        
        return base_confidence
    
    def _detect_potential_bias(self, synthesis_result: Dict[str, Any], ai_results: Dict[str, Any]) -> List[str]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–µ–π"""
        return []
    
    def _get_methodology_used(self, ai_results: Dict[str, Any]) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏"""
        methodology = []
        
        if "claude" in ai_results and ai_results["claude"].get("status") != "failed":
            methodology.append("Anthropic Claude 3.5 Sonnet - –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å–∏–Ω—Ç–µ–∑ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è")
        
        if "openai" in ai_results and ai_results["openai"].get("status") == "success":
            methodology.append("OpenAI GPT-4o - –º–Ω–æ–≥–æ–∞—Å–ø–µ–∫—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (Big Five + —ç–º–æ—Ü–∏–∏ + –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è)")
            methodology.append("IBM Research - –ø—Å–∏—Ö–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑")
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –æ–±–∞ —Å–µ—Ä–≤–∏—Å–∞
        if len([r for r in ai_results.values() if r.get("status") != "failed"]) > 1:
            methodology.append("–ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –º–µ–∂–¥—É AI —Å–∏—Å—Ç–µ–º–∞–º–∏")
        
        return methodology if methodology else ["–ë–∞–∑–æ–≤—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"]
    
    async def _create_analysis_record(self, analysis_input: AnalysisInput) -> int:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        return 1  # –ó–∞–≥–ª—É—à–∫–∞
    
    async def _save_analysis_result(self, analysis_id: int, result: AnalysisResult, ai_results: Dict[str, Any], synthesis_result: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        pass
    
    async def _save_analysis_error(self, analysis_id: int, service_name: str, error_message: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        pass
    
    def _format_modern_analysis_result(self, analysis_result: Dict[str, Any], successful_services: List[str], ai_results: Dict[str, Any]) -> str:
        """–ù–ê–£–ß–ù–û–ï —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø—Å–∏—Ö–æ–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ä—Ç—Ä–µ—Ç–∞ –ª–∏—á–Ω–æ—Å—Ç–∏"""
        
        if "error" in analysis_result or not analysis_result:
            return f"‚ö†Ô∏è **–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞**: {analysis_result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        scientific_metadata = analysis_result.get("scientific_metadata", {})
        comprehensive_analysis = analysis_result.get("comprehensive_personality_analysis", {})
        big_five_profile = analysis_result.get("big_five_scientific_profile", {})
        emotional_intelligence = analysis_result.get("emotional_intelligence_breakdown", {})
        cognitive_patterns = analysis_result.get("cognitive_behavioral_patterns", {})
        interpersonal_psychology = analysis_result.get("interpersonal_psychology", {})
        professional_profile = analysis_result.get("professional_psychological_profile", {})
        romantic_analysis = analysis_result.get("romantic_relationship_analysis", {})
        risk_assessment = analysis_result.get("risk_assessment_and_warnings", {})
        compatibility_matrix = analysis_result.get("compatibility_matrix", {})
        long_term_forecast = analysis_result.get("long_term_development_forecast", {})
        scientific_validation = analysis_result.get("scientific_validation", {})
        actionable_insights = analysis_result.get("actionable_insights_and_recommendations", {})
        
        confidence = analysis_result.get("confidence_score", 85)
        
        # === –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –ù–ê–£–ß–ù–û–ì–û –ü–û–†–¢–†–ï–¢–ê ===
        result = "# üìä –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ü–°–ò–•–û–ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ô –ü–û–†–¢–†–ï–¢ –õ–ò–ß–ù–û–°–¢–ò\n\n"
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        result += f"**–û–±—ä–µ–∫—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:** {scientific_metadata.get('analysis_subject', '–ê–Ω–æ–Ω–∏–º–Ω—ã–π —Å—É–±—ä–µ–∫—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è')}\n"
        result += f"**–û–±—ä–µ–º –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö:** {scientific_metadata.get('data_volume', 'N/A –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –µ–¥–∏–Ω–∏—Ü')}\n"
        result += f"**–ú–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞:** {', '.join(scientific_metadata.get('analysis_methods', ['–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ AI —Å–∏—Å—Ç–µ–º—ã']))}\n"
        result += f"**–ò–Ω–¥–µ–∫—Å –Ω–∞—É—á–Ω–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏:** {scientific_metadata.get('scientific_validity_index', f'{confidence}%')} (–≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏)\n"
        result += f"**–ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ä–µ–¥–∫–æ—Å—Ç—å:** {scientific_metadata.get('psychological_rarity', '–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—Å–∏—Ö–æ—Ç–∏–ø')}\n\n"
        result += "---\n\n"
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –ª–∏—á–Ω–æ—Å—Ç–∏
        if comprehensive_analysis:
            psychological_type = comprehensive_analysis.get("dominant_psychological_type", "")
            analytical_score = comprehensive_analysis.get("analytical_thinking_score", "")
            
            result += f"–ü–µ—Ä–µ–¥–æ –º–Ω–æ–π —Ä–∞–∑–≤–µ—Ä–Ω—É–ª–∞—Å—å –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∏–Ω–∞ {psychological_type}. "
            
            if analytical_score:
                result += f"–ê–Ω–∞–ª–∏–∑ —Ä–µ—á–µ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É IBM Watson Personality Insights –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç {analytical_score}. "
            
            cognitive_style = comprehensive_analysis.get("cognitive_processing_style", {})
            if cognitive_style:
                abstract_ratio = cognitive_style.get("abstract_vs_concrete_ratio", "")
                if abstract_ratio:
                    result += f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø–æ–Ω—è—Ç–∏–π —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {abstract_ratio}, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ–ø—É–ª—è—Ü–∏–æ–Ω–Ω—É—é –Ω–æ—Ä–º—É. "
                
                conceptual_level = cognitive_style.get("conceptual_thinking_level", "")
                if conceptual_level:
                    result += f"–£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è: {conceptual_level}. "
            
            lexical_analysis = comprehensive_analysis.get("lexical_analysis_insights", {})
            if lexical_analysis:
                complexity = lexical_analysis.get("complexity_indicators", "")
                if complexity:
                    result += f"–õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤—ã—è–≤–ª—è–µ—Ç {complexity}. "
                
                psychological_markers = lexical_analysis.get("psychological_markers", "")
                if psychological_markers:
                    result += f"–í —Ä–µ—á–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç {psychological_markers}. "
            
            result += "\n\n"
        
        # Big Five –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        if big_five_profile:
            result += "## üß¨ –ê–ù–ê–õ–ò–ó –õ–ò–ß–ù–û–°–¢–ò –ü–û –ú–û–î–ï–õ–ò \"–ë–û–õ–¨–®–ê–Ø –ü–Ø–¢–ï–†–ö–ê\"\n\n"
            
            traits_analysis = {
                "openness_to_experience": ("–û–¢–ö–†–´–¢–û–°–¢–¨ –ö –û–ü–´–¢–£", "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å"),
                "conscientiousness": ("–î–û–ë–†–û–°–û–í–ï–°–¢–ù–û–°–¢–¨", "–û—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ —Ü–µ–ª–µ—É—Å—Ç—Ä–µ–º–ª–µ–Ω–Ω–æ—Å—Ç—å"),
                "extraversion": ("–≠–ö–°–¢–†–ê–í–ï–†–°–ò–Ø", "–°–æ—Ü–∏–∞–ª—å–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è –∏ –æ–±—â–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"),
                "agreeableness": ("–î–û–ë–†–û–ñ–ï–õ–ê–¢–ï–õ–¨–ù–û–°–¢–¨", "–ö–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –¥–æ–≤–µ—Ä–∏–µ"),
                "neuroticism": ("–ù–ï–ô–†–û–¢–ò–ó–ú", "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å (–æ–±—Ä–∞—Ç–Ω–∞—è —à–∫–∞–ª–∞)")
            }
            
            for trait_key, (trait_name, trait_description) in traits_analysis.items():
                trait_data = big_five_profile.get(trait_key, {})
                if trait_data:
                    score = trait_data.get("score", "N/A")
                    percentile = trait_data.get("population_percentile", "")
                    markers = trait_data.get("cognitive_markers", "")
                    
                    result += f"**{trait_name}** ({trait_description}): {score}\n"
                    if percentile:
                        result += f"*–ü–æ–ø—É–ª—è—Ü–∏–æ–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è:* {percentile}\n"
                    if markers:
                        result += f"*–ú–∞—Ä–∫–µ—Ä—ã –≤ —Ç–µ–∫—Å—Ç–µ:* {markers}\n"
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è
                    if trait_key == "conscientiousness":
                        perfectionism = trait_data.get("perfectionism_index", "")
                        if perfectionism:
                            result += f"*–¢–∏–ø –ø–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏–∑–º–∞:* {perfectionism}\n"
                        anancast = trait_data.get("anancast_tendencies", "")
                        if anancast:
                            result += f"*–ê–Ω–∞–Ω–∫–∞—Å—Ç–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏:* {anancast}\n"
                    
                    elif trait_key == "extraversion":
                        social_type = trait_data.get("social_energy_type", "")
                        if social_type:
                            result += f"*–¢–∏–ø —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π —ç–Ω–µ—Ä–≥–∏–∏:* {social_type}\n"
                        communication = trait_data.get("communication_preference", "")
                        if communication:
                            result += f"*–ö–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è:* {communication}\n"
                    
                    result += "\n"
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
        if emotional_intelligence:
            result += "## üí≠ –ê–ù–ê–õ–ò–ó –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–û–ì–û –ò–ù–¢–ï–õ–õ–ï–ö–¢–ê\n\n"
            
            ei_components = {
                "self_awareness": "–°–∞–º–æ—Å–æ–∑–Ω–∞–Ω–∏–µ",
                "self_regulation": "–°–∞–º–æ—Ä–µ–≥—É–ª—è—Ü–∏—è", 
                "social_awareness": "–°–æ—Ü–∏–∞–ª—å–Ω–∞—è –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç—å",
                "relationship_management": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏"
            }
            
            for component_key, component_name in ei_components.items():
                score = emotional_intelligence.get(component_key, "")
                if score:
                    result += f"**{component_name}:** {score}\n"
            
            processing_speed = emotional_intelligence.get("emotional_processing_speed", "")
            if processing_speed:
                result += f"**–°–∫–æ—Ä–æ—Å—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {processing_speed}\n"
            
            complexity_tolerance = emotional_intelligence.get("emotional_complexity_tolerance", "")
            if complexity_tolerance:
                result += f"**–¢–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:** {complexity_tolerance}\n"
            
            result += "\n"
        
        # –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ-–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if cognitive_patterns:
            result += "## üéØ –ö–û–ì–ù–ò–¢–ò–í–ù–û-–ü–û–í–ï–î–ï–ù–ß–ï–°–ö–ò–ï –ü–ê–¢–¢–ï–†–ù–´\n\n"
            
            decision_making = cognitive_patterns.get("decision_making_style", {})
            if decision_making:
                result += "**–°—Ç–∏–ª—å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π:**\n"
                for key, value in decision_making.items():
                    if value:
                        key_readable = key.replace("_", " ").title()
                        result += f"‚Ä¢ {key_readable}: {value}\n"
                result += "\n"
            
            problem_solving = cognitive_patterns.get("problem_solving_approach", {})
            if problem_solving:
                result += "**–ü–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º:**\n"
                for key, value in problem_solving.items():
                    if value:
                        key_readable = key.replace("_", " ").title()
                        result += f"‚Ä¢ {key_readable}: {value}\n"
                result += "\n"
        
        # –ú–µ–∂–ª–∏—á–Ω–æ—Å—Ç–Ω–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è
        if interpersonal_psychology:
            result += "## üí´ –ú–ï–ñ–õ–ò–ß–ù–û–°–¢–ù–ê–Ø –ü–°–ò–•–û–õ–û–ì–ò–Ø\n\n"
            
            attachment = interpersonal_psychology.get("attachment_style", "")
            if attachment:
                result += f"**–°—Ç–∏–ª—å –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:** {attachment}\n"
            
            intimacy_pattern = interpersonal_psychology.get("intimacy_formation_pattern", "")
            if intimacy_pattern:
                result += f"**–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–ª–∏–∑–æ—Å—Ç–∏:** {intimacy_pattern}\n"
            
            boundaries = interpersonal_psychology.get("boundary_setting_ability", "")
            if boundaries:
                result += f"**–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü:** {boundaries}\n"
            
            conflict_tolerance = interpersonal_psychology.get("conflict_tolerance", "")
            if conflict_tolerance:
                result += f"**–¢–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º:** {conflict_tolerance}\n"
            
            result += "\n"
        
        # –†–æ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        if romantic_analysis:
            result += "## üíï –ê–ù–ê–õ–ò–ó –†–û–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–• –û–¢–ù–û–®–ï–ù–ò–ô\n\n"
            
            attachment_romance = romantic_analysis.get("attachment_in_romance", "")
            if attachment_romance:
                result += f"**–ü—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å –≤ —Ä–æ–º–∞–Ω—Ç–∏–∫–µ:** {attachment_romance}\n"
            
            love_languages = romantic_analysis.get("love_language_preferences", "")
            if love_languages:
                result += f"**–Ø–∑—ã–∫–∏ –ª—é–±–≤–∏:** {love_languages}\n"
            
            intimacy_pace = romantic_analysis.get("intimacy_development_pace", "")
            if intimacy_pace:
                result += f"**–¢–µ–º–ø —Ä–∞–∑–≤–∏—Ç–∏—è –±–ª–∏–∑–æ—Å—Ç–∏:** {intimacy_pace}\n"
            
            conflict_resolution = romantic_analysis.get("conflict_resolution_in_relationships", "")
            if conflict_resolution:
                result += f"**–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤:** {conflict_resolution}\n"
            
            compatibility_reqs = romantic_analysis.get("compatibility_requirements", "")
            if compatibility_reqs:
                result += f"**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏:** {compatibility_reqs}\n"
            
            result += "\n"
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if compatibility_matrix:
            result += "## üîó –ú–ê–¢–†–ò–¶–ê –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò\n\n"
            
            compatibility_types = {
                "analytical_types_compatibility": "–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∏–ø—ã (NT)",
                "creative_introverts_compatibility": "–¢–≤–æ—Ä—á–µ—Å–∫–∏–µ –∏–Ω—Ç—Ä–æ–≤–µ—Ä—Ç—ã (NF)",
                "extraverted_types_compatibility": "–≠–∫—Å—Ç—Ä–∞–≤–µ—Ä—Ç–Ω—ã–µ —Ç–∏–ø—ã",
                "traditional_types_compatibility": "–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ —Ç–∏–ø—ã (SJ)"
            }
            
            for compat_key, compat_name in compatibility_types.items():
                compat_score = compatibility_matrix.get(compat_key, "")
                if compat_score:
                    result += f"‚Ä¢ **{compat_name}:** {compat_score}\n"
            
            optimal_partner = compatibility_matrix.get("optimal_partner_profile", "")
            if optimal_partner:
                result += f"\n**–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –ø–∞—Ä—Ç–Ω–µ—Ä–∞:** {optimal_partner}\n"
            
            problematic = compatibility_matrix.get("problematic_combinations", "")
            if problematic:
                result += f"**–ü—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω—ã–µ —Å–æ—á–µ—Ç–∞–Ω–∏—è:** {problematic}\n"
            
            result += "\n"
        
        # –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑
        if long_term_forecast:
            result += "## üîÆ –î–û–õ–ì–û–°–†–û–ß–ù–´–ô –ü–†–û–ì–ù–û–ó –†–ê–ó–í–ò–¢–ò–Ø\n\n"
            
            professional_trajectory = long_term_forecast.get("five_year_professional_trajectory", "")
            if professional_trajectory:
                result += f"**5-–ª–µ—Ç–Ω—è—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è:** {professional_trajectory}\n"
            
            growth_opportunities = long_term_forecast.get("personal_growth_opportunities", "")
            if growth_opportunities:
                result += f"**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–∏—á–Ω–æ—Å—Ç–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞:** {growth_opportunities}\n"
            
            life_transitions = long_term_forecast.get("potential_life_transitions", "")
            if life_transitions:
                result += f"**–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã:** {life_transitions}\n"
            
            success_factors = long_term_forecast.get("success_probability_factors", "")
            if success_factors:
                result += f"**–§–∞–∫—Ç–æ—Ä—ã —É—Å–ø–µ—Ö–∞:** {success_factors}\n"
            
            result += "\n"
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤
        if risk_assessment:
            result += "## ‚ö†Ô∏è –ê–ù–ê–õ–ò–ó –†–ò–°–ö–û–í –ò –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø\n\n"
            
            primary_risks = risk_assessment.get("primary_psychological_risks", [])
            if primary_risks:
                result += "**–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏:**\n"
                for risk in primary_risks[:3]:
                    result += f"‚Ä¢ {risk}\n"
                result += "\n"
            
            burnout_info = risk_assessment.get("burnout_susceptibility", {})
            if burnout_info:
                result += "**–°–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –≤—ã–≥–æ—Ä–∞–Ω–∏—é:**\n"
                for key, value in burnout_info.items():
                    if value:
                        key_readable = key.replace("_", " ").title()
                        result += f"‚Ä¢ {key_readable}: {value}\n"
                result += "\n"
            
            early_warnings = risk_assessment.get("early_warning_signs", [])
            if early_warnings:
                result += "**–†–∞–Ω–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞—é—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã:**\n"
                for warning in early_warnings[:3]:
                    result += f"‚Ä¢ {warning}\n"
                result += "\n"
        
        # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if actionable_insights:
            result += "## üöÄ –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò\n\n"
            
            immediate_actions = actionable_insights.get("immediate_self_optimization", [])
            if immediate_actions:
                result += "**–ù–µ–º–µ–¥–ª–µ–Ω–Ω—ã–µ —à–∞–≥–∏ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**\n"
                for action in immediate_actions[:3]:
                    result += f"‚Ä¢ {action}\n"
                result += "\n"
            
            career_moves = actionable_insights.get("career_strategic_moves", [])
            if career_moves:
                result += "**–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ –∫–∞—Ä—å–µ—Ä–Ω—ã–µ —Ö–æ–¥—ã:**\n"
                for move in career_moves[:3]:
                    result += f"‚Ä¢ {move}\n"
                result += "\n"
            
            relationship_improvements = actionable_insights.get("relationship_improvement_tactics", [])
            if relationship_improvements:
                result += "**–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π:**\n"
                for improvement in relationship_improvements[:3]:
                    result += f"‚Ä¢ {improvement}\n"
                result += "\n"
        
        # –ù–∞—É—á–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        if scientific_validation:
            result += "## üî¨ –ù–ê–£–ß–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø\n\n"
            
            correlation = scientific_validation.get("cross_system_correlation", "")
            if correlation:
                result += f"**–ö—Ä–æ—Å—Å-—Å–∏—Å—Ç–µ–º–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è:** {correlation}\n"
            
            confidence_level = scientific_validation.get("confidence_level", "")
            if confidence_level:
                result += f"**–£—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏:** {confidence_level}\n"
            
            methodology_strengths = scientific_validation.get("methodology_strengths", "")
            if methodology_strengths:
                result += f"**–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏:** {methodology_strengths}\n"
            
            limitations = scientific_validation.get("methodological_limitations", "")
            if limitations:
                result += f"**–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:** {limitations}\n"
            
            cultural_notes = scientific_validation.get("cultural_adaptation_notes", "")
            if cultural_notes:
                result += f"**–ö—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:** {cultural_notes}\n"
            
            result += "\n"
        
        # –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
        result += "---\n\n"
        result += f"**üìä –ò–¢–û–ì–û–í–´–ô –ò–ù–î–ï–ö–° –î–û–°–¢–û–í–ï–†–ù–û–°–¢–ò:** {confidence}%\n\n"
        
        # AI —Å–∏—Å—Ç–µ–º—ã
        if len(successful_services) > 1:
            ai_names = []
            if "claude" in successful_services:
                ai_names.append("Claude 3.5 Sonnet")
            if "openai" in successful_services:
                ai_names.append("OpenAI GPT-4o")
            if "cohere" in successful_services:
                ai_names.append("Cohere Command-R+")
            if "huggingface" in successful_services:
                ai_names.append("HuggingFace Transformers")
            
            result += f"**ü§ñ AI –°–ò–°–¢–ï–ú–´:** {' + '.join(ai_names)}\n"
            result += f"**üî¨ –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø:** –ú—É–ª—å—Ç–∏-AI –∫–æ–Ω—Å–µ–Ω—Å—É—Å —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π ({len(successful_services)} —Å–∏—Å—Ç–µ–º)\n"
        else:
            result += f"**ü§ñ AI –î–í–ò–ñ–û–ö:** {successful_services[0].title()}\n"
            result += f"**üî¨ –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø:** –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑\n"
        
        result += f"\n*–î–∞–Ω–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–∞—É—á–Ω–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–∏–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ AI –∏ –º–æ–∂–µ—Ç —Å–ª—É–∂–∏—Ç—å –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ —Å—Ñ–µ—Ä–∞—Ö –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏ –ª–∏—á–Ω–æ—Å—Ç–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞.*\n\n"
        result += "üí¨ **–û—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞!**"
        
        return result


    # üî¨ –ù–û–í–´–ï –ú–ï–¢–û–î–´: –ù–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (2025)
    
    async def scientific_research_analysis(
        self, 
        person_data: Dict[str, Any], 
        user_id: int, 
        telegram_id: int
    ) -> str:
        """
        üî¨ –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–´–ô –ù–ê–£–ß–ù–û-–û–ë–û–°–ù–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–ó
        
        –°–æ–∑–¥–∞–µ—Ç –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π:
        - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –≤ PubMed, Google Scholar, –Ω–∞—É—á–Ω—ã—Ö –±–∞–∑–∞—Ö
        - –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ peer-reviewed –∏—Å—Ç–æ—á–Ω–∏–∫–∏  
        - –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π AI –∞–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
        - –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—ã–≤–æ–¥–æ–≤ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ AI —Å–∏—Å—Ç–µ–º
        
        Args:
            person_data: –î–∞–Ω–Ω—ã–µ –æ —á–µ–ª–æ–≤–µ–∫–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            telegram_id: Telegram ID
            
        Returns:
            –ù–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å
        """
        try:
            logger.info("üî¨ –ó–∞–ø—É—Å–∫ –Ω–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", 
                       user_id=user_id, 
                       telegram_id=telegram_id)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É PersonData
            person_obj = self._convert_to_person_data(person_data)
            
            # –≠—Ç–∞–ø 1: –ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π  
            logger.info("üìö –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π...")
            research_results = await self.research_engine.research_personality_profile(
                person_obj, max_sources=30
            )
            
            # –≠—Ç–∞–ø 2: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π AI –∞–Ω–∞–ª–∏–∑
            if research_results.get("sources") and len(research_results["sources"]) > 0:
                logger.info("üß† –ó–∞–ø—É—Å–∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ AI –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ–±—ä–µ–∫—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                from .scientific_research_engine import ScientificSource
                source_objects = [
                    ScientificSource(
                        title=source["title"],
                        authors=source["authors"],
                        publication=source["publication"],
                        year=source["year"],
                        doi=source.get("doi"),
                        pmid=source.get("pmid"),
                        url=source.get("url", ""),
                        abstract=source.get("abstract", ""),
                        citations=source.get("citations", 0),
                        quality_score=source.get("quality_score", 0.0),
                        source_type=source.get("source_type", "academic"),
                        language=source.get("language", "en")
                    )
                    for source in research_results["sources"]
                ]
                
                multi_ai_results = await self.multi_ai_analyzer.comprehensive_research_analysis(
                    person_obj, source_objects
                )
                
                return self._format_scientific_analysis_result(
                    research_results, multi_ai_results, person_obj
                )
            else:
                # –ï—Å–ª–∏ –Ω–∞—É—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –¥–µ–ª–∞–µ–º –æ–±—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                logger.warning("‚ö†Ô∏è –ù–∞—É—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –≤—ã–ø–æ–ª–Ω—è—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
                return await self._fallback_to_standard_analysis(person_data, user_id, telegram_id)
                
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –Ω–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", 
                        error=str(e), 
                        user_id=user_id,
                        exc_info=True)
            
            # Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É –ø—Ä–∏ –æ—à–∏–±–∫–µ
            return await self._fallback_to_standard_analysis(person_data, user_id, telegram_id)
    
    def _convert_to_person_data(self, data: Dict[str, Any]) -> PersonData:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –≤ –æ–±—ä–µ–∫—Ç PersonData"""
        return PersonData(
            name=data.get("name", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
            age=data.get("age"),
            gender=data.get("gender"),
            occupation=data.get("occupation", ""),
            behavior_description=data.get("behavior_description", ""),
            text_samples=data.get("text_samples", []),
            emotional_markers=data.get("emotional_markers", []),
            social_patterns=data.get("social_patterns", []),
            cognitive_traits=data.get("cognitive_traits", []),
            suspected_personality_type=data.get("suspected_personality_type", ""),
            country=data.get("country", "Russia"),
            cultural_context=data.get("cultural_context", "")
        )
    
    def _format_scientific_analysis_result(
        self, 
        research_results: Dict[str, Any],
        multi_ai_results: Dict[str, Any],
        person_data: PersonData
    ) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        import json
        
        comprehensive_profile = multi_ai_results.get("comprehensive_profile", "")
        individual_analyses = multi_ai_results.get("individual_analyses", [])
        analysis_metadata = multi_ai_results.get("analysis_metadata", {})
        research_summary = research_results.get("research_summary", {})
        
        result_text = f"""
## üî¨ –ù–ê–£–ß–ù–û-–û–ë–û–°–ù–û–í–ê–ù–ù–´–ô –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ü–†–û–§–ò–õ–¨

### üë§ –ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú–´–ô –°–£–ë–™–ï–ö–¢
**–ò–º—è:** {person_data.name}
**–í–æ–∑—Ä–∞—Å—Ç:** {person_data.age or '–ù–µ —É–∫–∞–∑–∞–Ω'}
**–ü—Ä–æ—Ñ–µ—Å—Å–∏—è:** {person_data.occupation or '–ù–µ —É–∫–∞–∑–∞–Ω–∞'}

### üìä –ù–ê–£–ß–ù–ê–Ø –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø
**üîç –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –±–∞–∑–∞:**
- **–ù–∞–π–¥–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π:** {research_summary.get('total_sources_found', 0)}
- **–ü—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é:** {research_summary.get('validated_sources', 0)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- **–ü–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:** {research_summary.get('queries_generated', 0)}

**üß† AI –∞–Ω–∞–ª–∏–∑:**
- **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ AI –º–æ–¥–µ–ª–µ–π:** {analysis_metadata.get('total_ai_models', 0)}
- **–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞:** {analysis_metadata.get('analysis_timestamp', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}

---

{comprehensive_profile}

---

### üîç –î–ï–¢–ê–õ–¨–ù–´–ï AI –ê–ù–ê–õ–ò–ó–´

"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç –∫–∞–∂–¥–æ–π AI —Å–∏—Å—Ç–µ–º—ã
        for i, analysis in enumerate(individual_analyses, 1):
            result_text += f"""
#### {i}. {analysis.get('ai_model', 'Unknown AI')} - {analysis.get('analysis_type', 'Unknown Type')}
**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {analysis.get('confidence_score', 0):.1%}
**–í—Ä–µ–º—è:** {analysis.get('timestamp', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}

**–û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã:**
```json
{json.dumps(analysis.get('findings', {}), indent=2, ensure_ascii=False)}
```

**–ù–∞—É—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏:** {', '.join(analysis.get('scientific_references', [])[:3])}

---
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—É—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        sources = research_results.get("sources", [])
        if sources:
            result_text += f"""

### üìö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ù–´–ï –ù–ê–£–ß–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò

"""
            for i, source in enumerate(sources[:10], 1):  # –¢–æ–ø 10 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                result_text += f"""
**{i}.** {source.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫')}
- **–ê–≤—Ç–æ—Ä—ã:** {', '.join(source.get('authors', []))}
- **–ü—É–±–ª–∏–∫–∞—Ü–∏—è:** {source.get('publication', '')} ({source.get('year', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –≥–æ–¥')})
- **–¢–∏–ø:** {source.get('source_type', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
- **–ö–∞—á–µ—Å—Ç–≤–æ:** {source.get('quality_score', 0):.1f}/100
- **DOI:** {source.get('doi', '–Ω–µ —É–∫–∞–∑–∞–Ω')}
- **URL:** {source.get('url', '–Ω–µ —É–∫–∞–∑–∞–Ω')}

"""
        
        result_text += f"""

### ‚öñÔ∏è –ù–ê–£–ß–ù–ê–Ø –í–ê–õ–ò–î–ù–û–°–¢–¨ –ò –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø

**‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–∞—É—á–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞:**
- –í—Å–µ –≤—ã–≤–æ–¥—ã –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ peer-reviewed –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö
- –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ AI —Å–∏—Å—Ç–µ–º
- –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ª–µ—Ç)
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

**‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**
- –ê–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ú–æ–∂–µ—Ç –æ—Ç—Ä–∞–∂–∞—Ç—å –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ
- –¢—Ä–µ–±—É–µ—Ç —É—á–µ—Ç–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π
- –ù–µ –∑–∞–º–µ–Ω—è–µ—Ç –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é

**üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–Ω—É—é —Ç–æ—á–∫—É –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏–∑—É—á–µ–Ω–∏—è
- –ö–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–π—Ç–µ—Å—å —Å –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∞–º–∏
- –£—á–∏—Ç—ã–≤–∞–π—Ç–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã–π –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
- –†–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–π—Ç–µ –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

---

### üìà –ú–ï–¢–ê-–ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê

**–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞—É—á–Ω–æ–π –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏:** {research_summary.get('validated_sources', 0) * 10}%
**–£—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è –∫ –≤—ã–≤–æ–¥–∞–º:** {analysis_metadata.get('total_ai_models', 1) * 25}%
**–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö:** –í—ã—Å–æ–∫–∞—è (–∏—Å—Ç–æ—á–Ω–∏–∫–∏ 2020-2025)

---

*üî¨ –ù–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–∑–¥–∞–Ω {datetime.now().strftime('%d.%m.%Y –≤ %H:%M')}*  
*üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {research_summary.get('validated_sources', 0)} –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤*  
*üß† –ó–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–æ {analysis_metadata.get('total_ai_models', 0)} AI —Å–∏—Å—Ç–µ–º*  
*‚ö° –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {research_summary.get('search_timestamp', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}*
"""
        
        return result_text
    
    async def _fallback_to_standard_analysis(
        self, 
        person_data: Dict[str, Any], 
        user_id: int, 
        telegram_id: int
    ) -> str:
        """Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –Ω–∞—É—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ —á–µ–ª–æ–≤–µ–∫–µ
        analysis_text = self._extract_text_from_person_data(person_data)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑
        standard_result = await self.quick_analyze(analysis_text, user_id, telegram_id)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ fallback
        return f"""
‚ö†Ô∏è **–ü–†–ò–ú–ï–ß–ê–ù–ò–ï:** –ù–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –í—ã–ø–æ–ª–Ω–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑.

---

{standard_result}

---

**–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—É—á–Ω–æ-–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:**
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É
- –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ API –∫–ª—é—á–µ–π –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –±–∞–∑
- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ–∑–∂–µ
"""
    
    def _extract_text_from_person_data(self, person_data: Dict[str, Any]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ —á–µ–ª–æ–≤–µ–∫–µ"""
        text_parts = []
        
        if person_data.get("behavior_description"):
            text_parts.append(f"–ü–æ–≤–µ–¥–µ–Ω–∏–µ: {person_data['behavior_description']}")
        
        if person_data.get("text_samples"):
            text_parts.append(f"–û–±—Ä–∞–∑—Ü—ã —Ç–µ–∫—Å—Ç–∞: {' '.join(person_data['text_samples'])}")
        
        if person_data.get("emotional_markers"):
            text_parts.append(f"–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: {', '.join(person_data['emotional_markers'])}")
        
        if person_data.get("social_patterns"):
            text_parts.append(f"–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {', '.join(person_data['social_patterns'])}")
        
        if person_data.get("cognitive_traits"):
            text_parts.append(f"–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: {', '.join(person_data['cognitive_traits'])}")
        
        return ". ".join(text_parts) if text_parts else "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–≤–∏–∂–∫–∞
analysis_engine = AnalysisEngine() 