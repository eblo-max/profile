"""AI service for text analysis using Claude Sonnet 4 via OpenRouter"""

import asyncio
import json
import time
from typing import Dict, Any, Optional, List

import httpx
from loguru import logger

from app.core.config import settings
from app.core.redis import redis_client
from app.utils.exceptions import AIServiceError
from app.utils.helpers import safe_json_loads, create_cache_key, extract_json_from_text
from app.prompts.analysis_prompts import (
    ANALYSIS_SYSTEM_PROMPT,
    PROFILER_SYSTEM_PROMPT, 
    COMPATIBILITY_SYSTEM_PROMPT,
    get_text_analysis_prompt,
    get_profiler_prompt,
    get_compatibility_prompt
)
from app.utils.enums import UrgencyLevel
import traceback


class AIService:
    """–ü—Ä–æ—Å—Ç–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π AI —Å–µ—Ä–≤–∏—Å —Å Claude Sonnet 4"""
    
    def __init__(self):
        # OpenRouter API configuration
        self.openrouter_api_key = settings.OPENAI_API_KEY  # –ò—Å–ø–æ–ª—å–∑—É–µ–º OPENAI_API_KEY –¥–ª—è OpenRouter
        self.openrouter_base_url = "https://openrouter.ai/api/v1"
        self.model = "anthropic/claude-sonnet-4"
        
        # Request limiting
        self._request_semaphore = asyncio.Semaphore(settings.MAX_CONCURRENT_AI_REQUESTS)
        self._last_request_time = 0
        self._last_model_used = self.model
        
        logger.info(f"‚úÖ AIService initialized with {self.model}")
    
    def _get_last_model_used(self) -> str:
        """Get the last model used for analysis"""
        return self._last_model_used
    
    async def _rate_limit(self):
        """Simple rate limiting"""
        current_time = time.time()
        time_since_last = current_time - self._last_request_time
        if time_since_last < settings.AI_RATE_LIMIT_SECONDS:
            await asyncio.sleep(settings.AI_RATE_LIMIT_SECONDS - time_since_last)
        self._last_request_time = time.time()
    
    async def _get_ai_response(
        self,
        system_prompt: str,
        user_prompt: str,
        response_format: str = "text",
        max_tokens: int = 4000,
        temperature: float = 0.7
    ) -> str:
        """Get response from Claude Sonnet 4 via OpenRouter"""
        
        if not self.openrouter_api_key:
            raise AIServiceError("OpenRouter API key –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        if not self.openrouter_api_key.startswith('sk-or-'):
            raise AIServiceError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç OpenRouter API –∫–ª—é—á–∞")
        
        await self._rate_limit()
        
        headers = {
            "Authorization": f"Bearer {self.openrouter_api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://psychodetective.ai",
            "X-Title": "PsychoDetective AI"
        }
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        data = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": False
        }
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.openrouter_base_url}/chat/completions",
                    headers=headers,
                    json=data
                )
                
                if response.status_code != 200:
                    error_text = response.text
                    logger.error(f"OpenRouter API error {response.status_code}: {error_text}")
                    raise AIServiceError(f"OpenRouter API –æ—à–∏–±–∫–∞: {response.status_code}")
                
                result = response.json()
                
                if 'choices' not in result or not result['choices']:
                    logger.error(f"Invalid OpenRouter response: {result}")
                    raise AIServiceError("–ù–µ–≤–µ—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç OpenRouter API")
                
                content = result['choices'][0]['message']['content']
                logger.info(f"‚úÖ OpenRouter response received: {len(content)} chars")
                
                return content
                
        except httpx.TimeoutException:
            logger.error("OpenRouter API timeout")
            raise AIServiceError("–¢–∞–π–º–∞—É—Ç OpenRouter API")
        except Exception as e:
            logger.error(f"OpenRouter API error: {e}")
            raise AIServiceError(f"–û—à–∏–±–∫–∞ OpenRouter API: {str(e)}")
    
    async def analyze_text(
        self,
        text: str,
        analysis_type: str = "general",
        user_id: Optional[int] = None,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        start_time = time.time()
        
        # Cache key
        cache_key = create_cache_key("text_analysis", user_id or 0, hash(text + analysis_type))
        
        if use_cache:
            cached_result = await redis_client.get(cache_key)
            if cached_result:
                logger.info(f"üì¶ Text analysis cache hit")
                return cached_result
        
        try:
            # Create prompts
            system_prompt = ANALYSIS_SYSTEM_PROMPT
            user_prompt = get_text_analysis_prompt(text, analysis_type)
            
            # Get AI response
            async with self._request_semaphore:
                response = await self._get_ai_response(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    response_format="json",
                    max_tokens=3000
                )
            
            # Parse response
            result = self._parse_analysis_response(response)
            result["processing_time"] = time.time() - start_time
            result["ai_model_used"] = self._get_last_model_used()
            
            # Cache result
            if use_cache:
                await redis_client.set(cache_key, result, expire=settings.ANALYSIS_CACHE_TTL)
            
            logger.info(f"‚úÖ Text analysis completed in {result['processing_time']:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Text analysis failed: {e}")
            raise AIServiceError(f"–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–µ —É–¥–∞–ª—Å—è: {str(e)}")
    
    async def profile_partner(
        self,
        answers: List[Dict[str, Any]],
        user_id: int,
        partner_name: str = "–ø–∞—Ä—Ç–Ω–µ—Ä",
        partner_description: str = "",
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Ç–Ω–µ—Ä–∞"""
        start_time = time.time()
        
        # Format answers
        if isinstance(answers, list):
            answers_text = []
            for answer in answers:
                question_text = answer.get('question', f"Question {answer.get('question_id', 'N/A')}")
                answer_text = answer.get('answer', 'No answer')
                answers_text.append(f"Q: {question_text}\nA: {answer_text}")
            combined_answers = "\n\n".join(answers_text)
        else:
            combined_answers = str(answers)
        
        # Cache key
        cache_key = create_cache_key("profile_analysis", user_id, hash(combined_answers + partner_name))
        
        if use_cache:
            cached_result = await redis_client.get(cache_key)
            if cached_result:
                logger.info(f"üì¶ Profile analysis cache hit for user {user_id}")
                return cached_result
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analysis_data = {
                'questionnaire_data': {
                    f'question_{i}': {
                        'question': answer.get('question', f'–í–æ–ø—Ä–æ—Å {i}'),
                        'answer': answer.get('answer', str(answer))
                    } for i, answer in enumerate(answers, 1)
                },
                'partner_name': partner_name,
                'partner_description': partner_description
            }
            
            # Create enhanced prompts
            user_prompt = self._create_enhanced_user_prompt(analysis_data)
            
            # Get analysis from Claude Sonnet 4
            async with self._request_semaphore:
                response = await self._get_ai_response(
                    system_prompt="",  # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –≤–∫–ª—é—á–µ–Ω –≤ user_prompt
                    user_prompt=user_prompt,
                    response_format="text",  # –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ text –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    max_tokens=8000,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    temperature=0.7  # –ë–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                )
            
            # Parse response - —Ç–µ–ø–µ—Ä—å —ç—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
            result = {
                "psychological_profile": response,
                "processing_time": time.time() - start_time,
                "ai_model_used": self._get_last_model_used(),
                "analysis_mode": "detailed_portrait",
                "cost_estimate": 0.15,  # –£–≤–µ–ª–∏—á–µ–Ω–∞ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏–∑-–∑–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                "word_count": len(response.split()),
                "character_count": len(response)
            }
            
            # Cache result
            if use_cache:
                await redis_client.set(cache_key, result, expire=settings.ANALYSIS_CACHE_TTL)
            
            logger.info(f"‚úÖ Profile analysis completed in {result['processing_time']:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Profile analysis failed: {e}")
            raise AIServiceError(f"–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å: {str(e)}")
    
    async def check_compatibility(
        self,
        user_profile: Dict[str, Any],
        partner_profile: Dict[str, Any],
        user_id: Optional[int] = None,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        start_time = time.time()
        
        # Cache key
        cache_key = create_cache_key("compatibility", user_id or 0, 
                                   hash(str(user_profile) + str(partner_profile)))
        
        if use_cache:
            cached_result = await redis_client.get(cache_key)
            if cached_result:
                logger.info(f"üì¶ Compatibility analysis cache hit")
                return cached_result
        
        try:
            # Create prompts
            system_prompt = COMPATIBILITY_SYSTEM_PROMPT
            user_prompt = get_compatibility_prompt(user_profile, partner_profile)
            
            # Get AI response
            async with self._request_semaphore:
                response = await self._get_ai_response(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    response_format="json",
                    max_tokens=3000
                )
            
            # Parse response
            result = self._parse_compatibility_response(response)
            result["processing_time"] = time.time() - start_time
            result["ai_model_used"] = self._get_last_model_used()
            
            # Cache result
            if use_cache:
                await redis_client.set(cache_key, result, expire=settings.ANALYSIS_CACHE_TTL)
            
            logger.info(f"‚úÖ Compatibility analysis completed in {result['processing_time']:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Compatibility analysis failed: {e}")
            raise AIServiceError(f"–ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –Ω–µ —É–¥–∞–ª—Å—è: {str(e)}")
    
    def _create_enhanced_system_prompt(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç"""
        return """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–ø—Å–∏—Ö–æ–ª–æ–≥ —Å 15-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞ –ª–∏—á–Ω–æ—Å—Ç–∏ –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π. 

–¢–≤–æ—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:
- –ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤ –ª–∏—á–Ω–æ—Å—Ç–∏
- –°–µ–º–µ–π–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è –∏ –∞–Ω–∞–ª–∏–∑ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π  
- –ö—Ä–∏–º–∏–Ω–∞–ª—å–Ω–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è –∏ –ø—Ä–æ—Ñ–∞–π–ª–∏–Ω–≥
- –ù–µ–π—Ä–æ–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è –∏ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑

–í–ê–ñ–ù–û: 
- –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
- –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–µ–Ω –∏ –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω –≤ –æ—Ü–µ–Ω–∫–∞—Ö
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
- –£–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –∫—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏"""
    
    def _create_enhanced_user_prompt(self, analysis_data: dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∫–µ—Ç—ã"""
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        system_prompt = """
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫—Ä–∏–º–∏–Ω–∞–ª—å–Ω–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ –∏ –ø—Ä–æ—Ñ–∞–π–ª–∏–Ω–≥—É —Å 20-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã —Å —Ç–æ–∫—Å–∏—á–Ω—ã–º–∏ –ª–∏—á–Ω–æ—Å—Ç—è–º–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞–≤–∞—Ç—å –≥–ª—É–±–æ–∫–∏–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä—Ç—Ä–µ—Ç—ã –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã.

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –ü–û–†–¢–†–ï–¢–£:

1. –û–ë–™–ï–ú: 2000-2500 —Å–ª–æ–≤ (—ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ!)
2. –Ø–ó–´–ö: –ù–∞—É—á–Ω–æ-–ø–æ–ø—É–ª—è—Ä–Ω—ã–π, –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ä—É—Å—Å–∫–æ–º—É —á–∏—Ç–∞—Ç–µ–ª—é
3. –°–¢–†–£–ö–¢–£–†–ê: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å —ç–º–æ–¥–∑–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
4. –ü–†–ò–ú–ï–†–´: –ú–∏–Ω–∏–º—É–º 5-7 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–π —Å –∏–º–µ–Ω–∞–º–∏ –∏ –¥–µ—Ç–∞–ª—è–º–∏
5. –ù–ê–£–ß–ù–û–°–¢–¨: –°—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏ –º–æ–¥–µ–ª–∏

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê –ü–û–†–¢–†–ï–¢–ê:

–ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ü–û–†–¢–†–ï–¢: "[–¢–∏–ø –ª–∏—á–Ω–æ—Å—Ç–∏]"

–û–ë–©–ê–Ø –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ê –õ–ò–ß–ù–û–°–¢–ò
- –ü–µ—Ä–≤–æ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ vs —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å
- –û—Ü–µ–Ω–∫–∞ –ø–æ –º–æ–¥–µ–ª–∏ "–¢–µ–º–Ω–æ–π —Ç—Ä–∏–∞–¥—ã" (–Ω–∞—Ä—Ü–∏—Å—Å–∏–∑–º/–º–∞–∫–∏–∞–≤–µ–ª–ª–∏–∑–º/–ø—Å–∏—Ö–æ–ø–∞—Ç–∏—è)
- –ö–ª—é—á–µ–≤—ã–µ –ª–∏—á–Ω–æ—Å—Ç–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

–û–°–ù–û–í–ù–û–ô –ü–ê–¢–¢–ï–†–ù –ü–û–í–ï–î–ï–ù–ò–Ø –í –û–¢–ù–û–®–ï–ù–ò–Ø–•
- –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–π —Ç–æ–∫—Å–∏—á–Ω–æ–π —á–µ—Ä—Ç—ã
- 2-3 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞ —Å –∏–º–µ–Ω–∞–º–∏ –∏ —Å–∏—Ç—É–∞—Ü–∏—è–º–∏
- –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã

–ö–û–ù–¢–†–û–õ–ò–†–£–Æ–©–ï–ï –ü–û–í–ï–î–ï–ù–ò–ï
- –≠–≤–æ–ª—é—Ü–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—è: –æ—Ç "–∑–∞–±–æ—Ç—ã" –∫ —Ç–æ—Ç–∞–ª—å–Ω–æ–º—É –Ω–∞–¥–∑–æ—Ä—É
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è
- –ò—Å—Ç–æ—Ä–∏—è –∏–∑ –∂–∏–∑–Ω–∏ —Å –¥–µ—Ç–∞–ª—è–º–∏

–ú–ê–ù–ò–ü–£–õ–Ø–¢–ò–í–ù–´–ï –¢–ï–•–ù–ò–ö–ò
- –ì–∞–∑–ª–∞–π—Ç–∏–Ω–≥ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ —Ñ—Ä–∞–∑
- –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —à–∞–Ω—Ç–∞–∂
- –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤

–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –î–ò–ó–†–ï–ì–£–õ–Ø–¶–ò–Ø
- –ù–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å —Ä–µ–∞–∫—Ü–∏–π
- –î–≤–æ–π–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã
- –ü—Ä–æ–µ–∫—Ü–∏—è –∏ –æ–±–≤–∏–Ω–µ–Ω–∏—è

–ò–ù–¢–ò–ú–ù–û–°–¢–¨ –ö–ê–ö –ò–ù–°–¢–†–£–ú–ï–ù–¢
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–ª–∏–∑–æ—Å—Ç–∏ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è
- –¢—Ä–∞–≤–º–∞—Ç–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã

–°–û–¶–ò–ê–õ–¨–ù–ê–Ø –ú–ê–°–ö–ê
- –ü—É–±–ª–∏—á–Ω—ã–π –æ–±—Ä–∞–∑ vs —á–∞—Å—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
- –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –¥–∏—Å—Å–æ–Ω–∞–Ω—Å —É –ø–∞—Ä—Ç–Ω–µ—Ä–∞
- –ü—Ä–∏–º–µ—Ä—ã –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏

–ü–†–û–ì–ù–û–ó –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
- –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º
- –¶–∏–∫–ª–∏—á–Ω–æ—Å—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–∞—Ä—Ç–Ω–µ—Ä–∞–º

–°–¢–ò–õ–ò–°–¢–ò–ß–ï–°–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:

1. –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö (–ê–Ω–Ω–∞, –ú–∞—Ä–∏–Ω–∞, –ï–ª–µ–Ω–∞ –∏ —Ç.–¥.)
2. –í–∫–ª—é—á–∞–π –ø—Ä—è–º—É—é —Ä–µ—á—å –∏ —Ü–∏—Ç–∞—Ç—ã ("—Ñ—Ä–∞–∑—ã –≤ –∫–∞–≤—ã—á–∫–∞—Ö")
3. –û–ø–∏—Å—ã–≤–∞–π —Ñ–∏–∑–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∞–∫—Ü–∏–∏ –∏ –Ω–µ–≤–µ—Ä–±–∞–ª–∏–∫—É
4. –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ –∞–≤—Ç–æ—Ä–æ–≤ (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ)
5. –ò—Å–ø–æ–ª—å–∑—É–π –º–µ—Ç–∞—Ñ–æ—Ä—ã –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
6. –ß–µ—Ä–µ–¥—É–π –¥–ª–∏–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞–±–∑–∞—Ü—ã —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –≤—ã–≤–æ–¥–∞–º–∏

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –ö–û–ù–¶–ï–ü–¶–ò–ò:
- –ú–æ–¥–µ–ª—å "–¢–µ–º–Ω–æ–π —Ç—Ä–∏–∞–¥—ã"
- –¢–µ–æ—Ä–∏—è –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç–∏
- –¶–∏–∫–ª –∞–±—å—é–∑–∞
- –ì–∞–∑–ª–∞–π—Ç–∏–Ω–≥
- –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –¥–∏–∑—Ä–µ–≥—É–ª—è—Ü–∏—è
- –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –¥–∏—Å—Å–æ–Ω–∞–Ω—Å
- –ü—Ä–æ–µ–∫—Ü–∏—è –∏ –¥—Ä—É–≥–∏–µ –∑–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã

–ó–ê–ü–†–ï–©–ï–ù–û:
- –ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ –æ–±–æ–±—â–µ–Ω–∏—è
- –ö–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã –±–µ–∑ –ø—Ä–∏–º–µ—Ä–æ–≤
- –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏
- –ò–∑–ª–∏—à–Ω—è—è –Ω–∞—É—á–Ω–æ—Å—Ç—å –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
- –ü–æ—Ä—Ç—Ä–µ—Ç—ã –∫–æ—Ä–æ—á–µ 2000 —Å–ª–æ–≤
"""

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        answers_text = ""
        partner_name = "–ø–∞—Ä—Ç–Ω–µ—Ä"
        partner_description = ""
        
        questionnaire_data = analysis_data.get('questionnaire_data', {})
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –ø–∞—Ä—Ç–Ω–µ—Ä–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'partner_name' in analysis_data:
            partner_name = analysis_data['partner_name']
        if 'partner_description' in analysis_data:
            partner_description = analysis_data['partner_description']
        
        for question_id, answer_data in questionnaire_data.items():
            if isinstance(answer_data, dict):
                question = answer_data.get('question', f'–í–æ–ø—Ä–æ—Å {question_id}')
                answer = answer_data.get('answer', '–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞')
                answers_text += f"–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç: {answer}\n\n"
            else:
                answers_text += f"–í–æ–ø—Ä–æ—Å {question_id}: {answer_data}\n\n"
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç —Å –¥–∞–Ω–Ω—ã–º–∏
        user_prompt = f"""
–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã —Å–æ–∑–¥–∞–π –≥–ª—É–±–æ–∫–∏–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—Ç—Ä–µ—Ç –ø–∞—Ä—Ç–Ω–µ—Ä–∞.

–ü–ê–†–¢–ù–ï–†: {partner_name} ({partner_description})

[–û–¢–í–ï–¢–´ –ù–ê –í–û–ü–†–û–°–´]
{answers_text}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:

1. –û–ë–™–ï–ú: –°–¢–†–û–ì–û 2400-2700 —Å–ª–æ–≤ (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ø—Ä–æ–≤–µ—Ä—è–π –∏—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ!)
2. –ü–ï–†–°–û–ù–ê–õ–ò–ó–ê–¶–ò–Ø: –£–ø–æ–º–∏–Ω–∞–π –∏–º—è {partner_name} –º–∏–Ω–∏–º—É–º 10-12 —Ä–∞–∑ –≤ —Ç–µ–∫—Å—Ç–µ
3. –¶–ò–¢–ê–¢–´: –ò—Å–ø–æ–ª—å–∑—É–π –ü–†–Ø–ú–´–ï —Ü–∏—Ç–∞—Ç—ã –∏–∑ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –º–∏–Ω–∏–º—É–º 15-20 —Ä–∞–∑
4. –°–¢–†–£–ö–¢–£–†–ê: –ë–ï–ó —Å–º–∞–π–ª–∏–∫–æ–≤, —Ä–µ—à–µ—Ç–æ–∫ (#), –∑–≤–µ–∑–¥–æ—á–µ–∫ (*) - —Ç–æ–ª—å–∫–æ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã
5. –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–ò–ó–ú: –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–π —Å—Ç–∏–ª—å, –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
6. –î–ï–¢–ê–õ–ò–ó–ê–¶–ò–Ø: –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª –º–∏–Ω–∏–º—É–º 350-400 —Å–ª–æ–≤ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê (2400-2700 —Å–ª–æ–≤):

–ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ü–û–†–¢–†–ï–¢: [–¢–∏–ø –ª–∏—á–Ω–æ—Å—Ç–∏ {partner_name}]

–û–ë–©–ê–Ø –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ê –õ–ò–ß–ù–û–°–¢–ò (400-450 —Å–ª–æ–≤)
- –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –∏–º–µ–Ω–∏ {partner_name}
- –ú–æ–¥–µ–ª—å "–¢–µ–º–Ω–æ–π —Ç—Ä–∏–∞–¥—ã" —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –±–∞–ª–ª–∞–º–∏
- –ü–µ—Ä–≤–æ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ vs —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
- –í–ª–∏—è–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –Ω–∞ —Ç–æ–∫—Å–∏—á–Ω—ã–µ —á–µ—Ä—Ç—ã

–î–û–ú–ò–ù–ò–†–£–Æ–©–ò–ï –ü–û–í–ï–î–ï–ù–ß–ï–°–ö–ò–ï –ü–ê–¢–¢–ï–†–ù–´ (450-500 —Å–ª–æ–≤)
- –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ–∫—Å–∏—á–Ω—ã–µ —á–µ—Ä—Ç—ã {partner_name}
- 4-5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –∏–º–µ–Ω–∞–º–∏ –ø–∞—Ä—Ç–Ω–µ—Ä—à
- –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã —Å –Ω–∞—É—á–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
- –≠–≤–æ–ª—é—Ü–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏

–°–ò–°–¢–ï–ú–ê –ö–û–ù–¢–†–û–õ–Ø –ò –ú–ê–ù–ò–ü–£–õ–Ø–¶–ò–ô (450-500 —Å–ª–æ–≤)
- –≠–≤–æ–ª—é—Ü–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—â–µ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è {partner_name}
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Å —Ü–∏—Ç–∞—Ç–∞–º–∏ –∏–∑ –æ—Ç–≤–µ—Ç–æ–≤
- –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π, —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π, —Å–æ—Ü–∏–∞–ª—å–Ω—ã–π, —Ü–∏—Ñ—Ä–æ–≤–æ–π –∫–æ–Ω—Ç—Ä–æ–ª—å
- –ò–∑–æ–ª—è—Ü–∏—è –æ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏

–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –î–ò–ó–†–ï–ì–£–õ–Ø–¶–ò–Ø –ò –ê–ì–†–ï–°–°–ò–Ø (400-450 —Å–ª–æ–≤)
- –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≥–Ω–µ–≤–∞ –∏ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏ {partner_name}
- –ü—Ä–æ–µ–∫—Ü–∏—è –∏ –¥–≤–æ–π–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã
- –í–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ –ø–∞—Ä—Ç–Ω–µ—Ä—à–∏
- –¶–∏–∫–ª—ã –Ω–∞—Å–∏–ª–∏—è –∏ –ø—Ä–∏–º–∏—Ä–µ–Ω–∏—è

–ò–ù–¢–ò–ú–ù–û–°–¢–¨ –ò –ù–ê–†–£–®–ï–ù–ò–ï –ì–†–ê–ù–ò–¶ (400-450 —Å–ª–æ–≤)
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–ª–∏–∑–æ—Å—Ç–∏ –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –≤–ª–∞—Å—Ç–∏
- –ü—Ä–∏–Ω—É–∂–¥–µ–Ω–∏–µ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —à–∞–Ω—Ç–∞–∂
- –¢—Ä–∞–≤–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–π
- –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –¥–ª—è –∂–µ—Ä—Ç–≤—ã

–°–û–¶–ò–ê–õ–¨–ù–ê–Ø –ú–ê–°–ö–ê –ò –ò–ó–û–õ–Ø–¶–ò–Ø (350-400 —Å–ª–æ–≤)
- –ü—É–±–ª–∏—á–Ω—ã–π –æ–±—Ä–∞–∑ vs —á–∞—Å—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ {partner_name}
- –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –¥–∏—Å—Å–æ–Ω–∞–Ω—Å —É –∂–µ—Ä—Ç–≤—ã
- –ò–∑–æ–ª—è—Ü–∏—è –æ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏
- –í–ª–∏—è–Ω–∏–µ –Ω–∞ –æ–∫—Ä—É–∂–µ–Ω–∏–µ

–ü–†–û–ì–ù–û–ó –ò –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò (450-500 —Å–ª–æ–≤)
- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–π {partner_name}
- –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –≤—ã—Ö–æ–¥–∞
- –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è
- –ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –≤–∏–∫—Ç–∏–º–∏–∑–∞—Ü–∏–∏

–°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª –Ω–∞—á–∏–Ω–∞–π —Å –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è {partner_name}
- –ò—Å–ø–æ–ª—å–∑—É–π —Ñ—Ä–∞–∑—ã: "–ü–æ–≤–µ–¥–µ–Ω–∏–µ {partner_name} —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞...", "{partner_name} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç..."
- –í–∫–ª—é—á–∞–π –ø—Ä—è–º—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ –æ—Ç–≤–µ—Ç–æ–≤: "–ö–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ –æ—Ç–≤–µ—Ç–∞—Ö: '[—Ü–∏—Ç–∞—Ç–∞]'"
- –î–æ–±–∞–≤–ª—è–π –Ω–∞—É—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏: "–°–æ–≥–ª–∞—Å–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º –¥–æ–∫—Ç–æ—Ä–∞ X..."
- –°–æ–∑–¥–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å –∏–º–µ–Ω–∞–º–∏: "–ê–Ω–Ω–∞, 28 –ª–µ—Ç, —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç..."

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –ü—Ä–æ–≤–µ—Ä—å –∏—Ç–æ–≥–æ–≤—ã–π –æ–±—ä–µ–º - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –°–¢–†–û–ì–û 2400-2700 —Å–ª–æ–≤!
- –£–±–µ—Ä–∏ –í–°–ï —Ä–µ—à–µ—Ç–∫–∏ (#), –∑–≤–µ–∑–¥–æ—á–∫–∏ (*) –∏ –¥—Ä—É–≥–∏–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
- –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–º
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–ø–æ–º—è–Ω–∏ –∏–º—è {partner_name} –≤ –∫–∞–∂–¥–æ–º —Ä–∞–∑–¥–µ–ª–µ
"""
        
        return system_prompt + "\n\n" + user_prompt
    
    def _parse_analysis_response(self, response: str) -> Dict[str, Any]:
        """Parse general analysis response"""
        try:
            data = extract_json_from_text(response)
            if not data:
                data = safe_json_loads(response, {})
            
            return {
                "analysis": data.get("analysis", "–ê–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"),
                "sentiment": data.get("sentiment", "neutral"),
                "key_points": data.get("key_points", []),
                "recommendations": data.get("recommendations", []),
                "confidence": float(data.get("confidence", 75.0))
            }
        except Exception as e:
            logger.error(f"Failed to parse analysis response: {e}")
            return {
                "analysis": "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞",
                "sentiment": "neutral", 
                "key_points": [],
                "recommendations": [],
                "confidence": 50.0
            }
    
    def _parse_profile_response(self, response: str) -> Dict[str, Any]:
        """Parse partner profile response"""
        try:
            data = extract_json_from_text(response)
            if not data:
                data = safe_json_loads(response, {})
            
            # Validate and extract all required fields
            result = {
                "psychological_profile": data.get("psychological_profile", "–ê–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"),
                "personality_traits": data.get("personality_traits", ["–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å", "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å"]),
                "behavioral_patterns": data.get("behavioral_patterns", ["–ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ"]),
                "red_flags": data.get("red_flags", []),
                "relationship_style": data.get("relationship_style", "–ó–¥–æ—Ä–æ–≤—ã–π —Å—Ç–∏–ª—å –æ—Ç–Ω–æ—à–µ–Ω–∏–π"),
                "strengths": data.get("strengths", ["–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å"]),
                "potential_challenges": data.get("potential_challenges", ["–¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è"]),
                "compatibility_factors": data.get("compatibility_factors", ["–û—Ç–∫—Ä—ã—Ç–æ—Å—Ç—å –∫ –æ–±—â–µ–Ω–∏—é"]),
                "recommendations": data.get("recommendations", ["–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ"]),
                "overall_risk_score": float(data.get("overall_risk_score", 25.0)),
                "dark_triad": data.get("dark_triad", {"narcissism": 2.0, "machiavellianism": 1.5, "psychopathy": 1.0}),
                "confidence_level": float(data.get("confidence_level", 85.0)),
                "summary": data.get("summary", "–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–¥–æ—Ä–æ–≤—É—é –ª–∏—á–Ω–æ—Å—Ç—å —Å —Ö–æ—Ä–æ—à–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º –¥–ª—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π."),
                "survival_guide": data.get("survival_guide", ["–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ"]),
                "safety_alerts": data.get("safety_alerts", []),
                "urgency_level": data.get("urgency_level", "LOW"),
                "block_scores": data.get("block_scores", {
                    "narcissism": 2.0, "control": 1.7, "gaslighting": 1.5, 
                    "emotion": 1.3, "intimacy": 1.0, "social": 1.4
                }),
                # –ù–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                "manipulation_tactics": data.get("manipulation_tactics", ["–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ"]),
                "escalation_triggers": data.get("escalation_triggers", ["–ö—Ä–∏—Ç–∏–∫–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è"]),
                "control_mechanisms": data.get("control_mechanisms", ["–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ–±—â–µ–Ω–∏—è"])
            }
            
            # Validate urgency level
            valid_urgency = ["LOW", "MEDIUM", "HIGH", "CRITICAL"]
            if result["urgency_level"] not in valid_urgency:
                result["urgency_level"] = "LOW"
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to parse profile response: {e}")
            return {
                "psychological_profile": "–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                "personality_traits": ["–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å", "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å"],
                "behavioral_patterns": ["–ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ"],
                "red_flags": [],
                "relationship_style": "–ó–¥–æ—Ä–æ–≤—ã–π —Å—Ç–∏–ª—å –æ—Ç–Ω–æ—à–µ–Ω–∏–π",
                "strengths": ["–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å"],
                "potential_challenges": ["–¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è"],
                "compatibility_factors": ["–û—Ç–∫—Ä—ã—Ç–æ—Å—Ç—å –∫ –æ–±—â–µ–Ω–∏—é"],
                "recommendations": ["–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ"],
                "overall_risk_score": 25.0,
                "dark_triad": {"narcissism": 2.0, "machiavellianism": 1.5, "psychopathy": 1.0},
                "confidence_level": 60.0,
                "summary": "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.",
                "survival_guide": ["–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ"],
                "safety_alerts": [],
                "urgency_level": "LOW",
                "block_scores": {"narcissism": 2.0, "control": 1.7, "gaslighting": 1.5, "emotion": 1.3, "intimacy": 1.0, "social": 1.4},
                "manipulation_tactics": ["–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ"],
                "escalation_triggers": ["–ö—Ä–∏—Ç–∏–∫–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è"],
                "control_mechanisms": ["–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ–±—â–µ–Ω–∏—è"]
            }
    
    def _parse_compatibility_response(self, response: str) -> Dict[str, Any]:
        """Parse compatibility analysis response"""
        try:
            data = extract_json_from_text(response)
            if not data:
                data = safe_json_loads(response, {})
            
            return {
                "compatibility_score": float(data.get("compatibility_score", 75.0)),
                "strengths": data.get("strengths", ["–û–±—â–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã"]),
                "challenges": data.get("challenges", ["–†–∞–∑–ª–∏—á–∏—è –≤ –ø–æ–¥—Ö–æ–¥–∞—Ö"]),
                "recommendations": data.get("recommendations", ["–û—Ç–∫—Ä—ã—Ç–æ–µ –æ–±—â–µ–Ω–∏–µ"]),
                "long_term_potential": data.get("long_term_potential", "–•–æ—Ä–æ—à–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª"),
                "areas_to_work_on": data.get("areas_to_work_on", ["–í–∑–∞–∏–º–æ–ø–æ–Ω–∏–º–∞–Ω–∏–µ"]),
                "summary": data.get("summary", "–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –Ω–∞ —Ö–æ—Ä–æ—à–µ–º —É—Ä–æ–≤–Ω–µ")
            }
        except Exception as e:
            logger.error(f"Failed to parse compatibility response: {e}")
            return {
                "compatibility_score": 70.0,
                "strengths": ["–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è"],
                "challenges": ["–¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"],
                "recommendations": ["–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∏–∑—É—á–µ–Ω–∏–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"],
                "long_term_potential": "–¢—Ä–µ–±—É–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏",
                "areas_to_work_on": ["–í–∑–∞–∏–º–æ–ø–æ–Ω–∏–º–∞–Ω–∏–µ"],
                "summary": "–¢—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"
            }


# Global AI service instance
ai_service = AIService() 